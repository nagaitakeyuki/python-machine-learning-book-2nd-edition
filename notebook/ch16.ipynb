{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../../../movie_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words occurrences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:16:21\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Counting words occurrences')\n",
    "for i, review in enumerate(df['review']):\n",
    "    # あとで半角スペースで単語を区切れるように、\n",
    "    # 句読点などの周りに半角スペースを挿入。\n",
    "    text = ''.join([c if c not in punctuation else ' '+c+' '\n",
    "                               for c in review]).lower()\n",
    "    df.loc[i, 'review'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this movie is great ! '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"This movie is great!\"\n",
    "''.join([c if c not in punctuation else ' '+c+' ' for c in a]).lower()\n",
    "\n",
    "#for c in a:\n",
    "#    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map reviews to ints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '.', ',', 'and', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:04\n"
     ]
    }
   ],
   "source": [
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "print(word_counts[:5])\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
    "\n",
    "mapped_reviews = []\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Map reviews to ints')\n",
    "for review in df['review']:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 200\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = review_arr[-sequence_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n",
      "[0 0 0 4 5]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((5,), dtype=int)\n",
    "print(a)\n",
    "b = np.array([1, 2, 3, 4, 5])\n",
    "a[-2:] = b[-2:]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequences[:25000, :]\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "\n",
    "X_test = sequences[25000:, :]\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x) // batch_size  # 切り捨て除算\n",
    "    x = x[:n_batches * batch_size]  # バッチサイズで割り切れない「はみ出し」を除外\n",
    "    if y is not None:\n",
    "        y = y[:n_batches * batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii : ii + batch_size], y[ii : ii + batch_size]\n",
    "        else:\n",
    "            yield x[ii : ii + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class SentimentRNN(object):\n",
    "    def __init__(self, n_words, seq_len=200,\n",
    "                            lstm_size=256, num_layers=1, batch_size=64,\n",
    "                            learning_rate=0.0001, embed_size=200):\n",
    "        self.n_words  = n_words\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers # LSTMのセル数\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "    def build(self):\n",
    "        tf_x = tf.placeholder(tf.int32,\n",
    "                                             shape=(self.batch_size, self.seq_len),\n",
    "                                             name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.float32,\n",
    "                                             shape=(self.batch_size),\n",
    "                                             name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32,\n",
    "                                                            name='tf_keepprob')\n",
    "        \n",
    "        embedding = tf.Variable(tf.random_uniform((self.n_words, self.embed_size),\n",
    "                                                                                     minval=-1, maxval=1),\n",
    "                                                   name='embedding')\n",
    "        \n",
    "        embed_x = tf.nn.embedding_lookup(embedding, tf_x, \n",
    "                                                                        name='embeded_x')\n",
    "        \n",
    "        print('  << embed_x >>  ', embed_x)\n",
    "        \n",
    "        # ここでいうセルとは、RNNの層のことっぽい\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "            [tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "                output_keep_prob=tf_keepprob)\n",
    "             \n",
    "            for i in range(self.num_layers)])\n",
    "        \n",
    "        self.initial_state = cells.zero_state(self.batch_size, tf.float32)\n",
    "        print('  << initial state >>  ', self.initial_state)\n",
    "        \n",
    "        # 埋め込みデータ、RNNセル、セルの初期状態をもとに、LSTMのパイプラインを作成する。\n",
    "        lstm_outputs, self.final_state = \\\n",
    "            tf.nn.dynamic_rnn(cells, embed_x, initial_state=self.initial_state)\n",
    "        \n",
    "        # lstm_outputsは、各タイムステップにおける、隠れ層ユニットの出力。ユニット数だけ存在する。\n",
    "        # 形状は(batch_size, タイムステップ数, 隠れ層のユニット数（lstm_size）)\n",
    "        # \n",
    "        # 参考：　https://jp.mathworks.com/help/deeplearning/ug/long-short-term-memory-networks.html;jsessionid=72c069a296d252b44e0d114132f6\n",
    "        # 参考：　https://orizuru.io/blog/machine-learning/lstm/\n",
    "        print('\\n  << lstm_output   >>  ', lstm_outputs)\n",
    "        \n",
    "        # final_stateは、最後のタイムステップにおける、「隠れ層の出力」と「セル状態」がセットになったもの。\n",
    "        # 参考：　https://stackoverflow.com/questions/49969349/hidden-states-vs-final-state-returned-by-tensorflows-dynamic-rnn\n",
    "        print('\\n  << final state   >>  ', self.final_state)\n",
    "        \n",
    "        # final_stateを次のミニバッチの初期状態にする理由は、\n",
    "        # ミニバッチ間の同一インデックスのデータどうしが系列関係にあって、\n",
    "        # その関係も考慮に入れて学習させるため。\n",
    "        # だが、今回の場合はそういった関係性はなさそうなので、本来は不要そう。\n",
    "        \n",
    "        logits = tf.layers.dense(inputs=lstm_outputs[:, -1],\n",
    "                                                units=1, activation=None,\n",
    "                                                name='logits')\n",
    "        \n",
    "        print('\\n  << logits  >>  ', logits)\n",
    "        \n",
    "        # サイズが１の次元を削除する。２次元→１次元（ベクトル）になる。\n",
    "        logits = tf.squeeze(logits, name='logits_squeezed')\n",
    "        print('\\n  << logits squeezed  >>  ', logits)\n",
    "        \n",
    "        y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
    "        \n",
    "        predictions = {\n",
    "            'probabilities': y_proba,\n",
    "            'labels': tf.cast(tf.round(y_proba), tf.int32, name='labels')\n",
    "        }\n",
    "        print('\\n  << predictions   >>  ', predictions)\n",
    "        \n",
    "        cost = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_y, logits=logits),\n",
    "            name='cost')\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name='train_op')\n",
    "    \n",
    "    def train(self, X_train, y_train, num_epochs):\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            iteration = 1\n",
    "            for epoch in range(num_epochs):\n",
    "                state = sess.run(self.initial_state)\n",
    "                \n",
    "                for batch_x, batch_y in create_batch_generator(\n",
    "                        X_train, y_train, self.batch_size):\n",
    "                    \n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                                   'tf_y:0': batch_y,\n",
    "                                   'tf_keepprob:0': 0.5,\n",
    "                                   self.initial_state: state}\n",
    "                    \n",
    "                    loss, _, state = sess.run(\n",
    "                        ['cost:0', 'train_op', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "                    \n",
    "                    if iteration % 20 == 0:\n",
    "                        print('Epochs: %d/%d Iteration: %d | Train loss: %.5f'\n",
    "                                     % (epoch + 1, num_epochs, iteration, loss))\n",
    "                        \n",
    "                    iteration += 1\n",
    "                \n",
    "                if(epoch+1) % 10 == 0:\n",
    "                    self.saver.save(sess, \"ch16-model/sentiment-%d.ckpt\" % epoch)\n",
    "        \n",
    "    # 予測のときにもミニバッチ間で最終状態の受け渡しをしている。\n",
    "    # あくまでも学習させたいのは重みであって、状態を学習させたいわけではない。\n",
    "    # ミニバッチ間の同一インデックスのデータどうしに系列関係がある場合は、\n",
    "    # 学習した重みを活用しつつ、状態を共有しないと正しい出力が出ない、ということ。\n",
    "    def predict(self, X_data, return_proba=False):\n",
    "        preds = []\n",
    "        with tf.Session(graph = self.g) as sess:\n",
    "            self.saver.restore(sess, tf.train.latest_checkpoint('./ch16-model/'))\n",
    "            test_state = sess.run(self.initial_state)\n",
    "        \n",
    "            for ii, batch_x in enumerate(create_batch_generator(\n",
    "                                                                X_data, \n",
    "                                                                None, \n",
    "                                                                batch_size=self.batch_size),\n",
    "                                                             1):\n",
    "\n",
    "                feed = {'tf_x:0': batch_x, 'tf_keepprob:0': 1.0,\n",
    "                               self.initial_state: test_state}\n",
    "\n",
    "                if return_proba:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "                else:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['labels:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "\n",
    "                preds.append(pred)\n",
    "            \n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  << embed_x >>   Tensor(\"embeded_x/Identity:0\", shape=(100, 200, 256), dtype=float32)\n",
      "  << initial state >>   (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      "  << lstm_output   >>   Tensor(\"rnn/transpose_1:0\", shape=(100, 200, 128), dtype=float32)\n",
      "\n",
      "  << final state   >>   (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      "  << logits  >>   Tensor(\"logits/BiasAdd:0\", shape=(100, 1), dtype=float32)\n",
      "\n",
      "  << logits squeezed  >>   Tensor(\"logits_squeezed:0\", shape=(100,), dtype=float32)\n",
      "\n",
      "  << predictions   >>   {'probabilities': <tf.Tensor 'probabilities:0' shape=(100,) dtype=float32>, 'labels': <tf.Tensor 'labels:0' shape=(100,) dtype=int32>}\n"
     ]
    }
   ],
   "source": [
    "# 長さが200未満のシーケンスを0でパディングするために、+1している。\n",
    "# Undefinedみたいな語彙を１つ足してる感じだと思う。\n",
    "n_words = max(list(word_to_int.values())) + 1\n",
    "\n",
    "rnn = SentimentRNN(n_words=n_words,\n",
    "                                      seq_len=sequence_length,\n",
    "                                      embed_size=256,\n",
    "                                      lstm_size=128,\n",
    "                                      num_layers=1,\n",
    "                                      batch_size=100,\n",
    "                                      learning_rate=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102967"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1/40 Iteration: 20 | Train loss: 0.68144\n",
      "Epochs: 1/40 Iteration: 40 | Train loss: 0.62962\n",
      "Epochs: 1/40 Iteration: 60 | Train loss: 0.64887\n",
      "Epochs: 1/40 Iteration: 80 | Train loss: 0.62561\n",
      "Epochs: 1/40 Iteration: 100 | Train loss: 0.57090\n",
      "Epochs: 1/40 Iteration: 120 | Train loss: 0.53749\n",
      "Epochs: 1/40 Iteration: 140 | Train loss: 0.48158\n",
      "Epochs: 1/40 Iteration: 160 | Train loss: 0.45924\n",
      "Epochs: 1/40 Iteration: 180 | Train loss: 0.47754\n",
      "Epochs: 1/40 Iteration: 200 | Train loss: 0.44295\n",
      "Epochs: 1/40 Iteration: 220 | Train loss: 0.39990\n",
      "Epochs: 1/40 Iteration: 240 | Train loss: 0.41825\n",
      "Epochs: 2/40 Iteration: 260 | Train loss: 0.39764\n",
      "Epochs: 2/40 Iteration: 280 | Train loss: 0.41892\n",
      "Epochs: 2/40 Iteration: 300 | Train loss: 0.51972\n",
      "Epochs: 2/40 Iteration: 320 | Train loss: 0.36637\n",
      "Epochs: 2/40 Iteration: 340 | Train loss: 0.32235\n",
      "Epochs: 2/40 Iteration: 360 | Train loss: 0.22373\n",
      "Epochs: 2/40 Iteration: 380 | Train loss: 0.35304\n",
      "Epochs: 2/40 Iteration: 400 | Train loss: 0.33850\n",
      "Epochs: 2/40 Iteration: 420 | Train loss: 0.31675\n",
      "Epochs: 2/40 Iteration: 440 | Train loss: 0.34686\n",
      "Epochs: 2/40 Iteration: 460 | Train loss: 0.46058\n",
      "Epochs: 2/40 Iteration: 480 | Train loss: 0.43436\n",
      "Epochs: 2/40 Iteration: 500 | Train loss: 0.40827\n",
      "Epochs: 3/40 Iteration: 520 | Train loss: 0.33363\n",
      "Epochs: 3/40 Iteration: 540 | Train loss: 0.29428\n",
      "Epochs: 3/40 Iteration: 560 | Train loss: 0.35254\n",
      "Epochs: 3/40 Iteration: 580 | Train loss: 0.26032\n",
      "Epochs: 3/40 Iteration: 600 | Train loss: 0.25236\n",
      "Epochs: 3/40 Iteration: 620 | Train loss: 0.28599\n",
      "Epochs: 3/40 Iteration: 640 | Train loss: 0.26728\n",
      "Epochs: 3/40 Iteration: 660 | Train loss: 0.25481\n",
      "Epochs: 3/40 Iteration: 680 | Train loss: 0.30043\n",
      "Epochs: 3/40 Iteration: 700 | Train loss: 0.19362\n",
      "Epochs: 3/40 Iteration: 720 | Train loss: 0.10855\n",
      "Epochs: 3/40 Iteration: 740 | Train loss: 0.31522\n",
      "Epochs: 4/40 Iteration: 760 | Train loss: 0.23287\n",
      "Epochs: 4/40 Iteration: 780 | Train loss: 0.17688\n",
      "Epochs: 4/40 Iteration: 800 | Train loss: 0.24049\n",
      "Epochs: 4/40 Iteration: 820 | Train loss: 0.26041\n",
      "Epochs: 4/40 Iteration: 840 | Train loss: 0.16279\n",
      "Epochs: 4/40 Iteration: 860 | Train loss: 0.13861\n",
      "Epochs: 4/40 Iteration: 880 | Train loss: 0.23361\n",
      "Epochs: 4/40 Iteration: 900 | Train loss: 0.19373\n",
      "Epochs: 4/40 Iteration: 920 | Train loss: 0.14055\n",
      "Epochs: 4/40 Iteration: 940 | Train loss: 0.19509\n",
      "Epochs: 4/40 Iteration: 960 | Train loss: 0.25872\n",
      "Epochs: 4/40 Iteration: 980 | Train loss: 0.09738\n",
      "Epochs: 4/40 Iteration: 1000 | Train loss: 0.07308\n",
      "Epochs: 5/40 Iteration: 1020 | Train loss: 0.15828\n",
      "Epochs: 5/40 Iteration: 1040 | Train loss: 0.13354\n",
      "Epochs: 5/40 Iteration: 1060 | Train loss: 0.17385\n",
      "Epochs: 5/40 Iteration: 1080 | Train loss: 0.13984\n",
      "Epochs: 5/40 Iteration: 1100 | Train loss: 0.09049\n",
      "Epochs: 5/40 Iteration: 1120 | Train loss: 0.15089\n",
      "Epochs: 5/40 Iteration: 1140 | Train loss: 0.09362\n",
      "Epochs: 5/40 Iteration: 1160 | Train loss: 0.06185\n",
      "Epochs: 5/40 Iteration: 1180 | Train loss: 0.18015\n",
      "Epochs: 5/40 Iteration: 1200 | Train loss: 0.10360\n",
      "Epochs: 5/40 Iteration: 1220 | Train loss: 0.05685\n",
      "Epochs: 5/40 Iteration: 1240 | Train loss: 0.16852\n",
      "Epochs: 6/40 Iteration: 1260 | Train loss: 0.16729\n",
      "Epochs: 6/40 Iteration: 1280 | Train loss: 0.07806\n",
      "Epochs: 6/40 Iteration: 1300 | Train loss: 0.16032\n",
      "Epochs: 6/40 Iteration: 1320 | Train loss: 0.07452\n",
      "Epochs: 6/40 Iteration: 1340 | Train loss: 0.07441\n",
      "Epochs: 6/40 Iteration: 1360 | Train loss: 0.17976\n",
      "Epochs: 6/40 Iteration: 1380 | Train loss: 0.14780\n",
      "Epochs: 6/40 Iteration: 1400 | Train loss: 0.16247\n",
      "Epochs: 6/40 Iteration: 1420 | Train loss: 0.13915\n",
      "Epochs: 6/40 Iteration: 1440 | Train loss: 0.20900\n",
      "Epochs: 6/40 Iteration: 1460 | Train loss: 0.43385\n",
      "Epochs: 6/40 Iteration: 1480 | Train loss: 0.14482\n",
      "Epochs: 6/40 Iteration: 1500 | Train loss: 0.11927\n",
      "Epochs: 7/40 Iteration: 1520 | Train loss: 0.11996\n",
      "Epochs: 7/40 Iteration: 1540 | Train loss: 0.04298\n",
      "Epochs: 7/40 Iteration: 1560 | Train loss: 0.21984\n",
      "Epochs: 7/40 Iteration: 1580 | Train loss: 0.09610\n",
      "Epochs: 7/40 Iteration: 1600 | Train loss: 0.09196\n",
      "Epochs: 7/40 Iteration: 1620 | Train loss: 0.18932\n",
      "Epochs: 7/40 Iteration: 1640 | Train loss: 0.12906\n",
      "Epochs: 7/40 Iteration: 1660 | Train loss: 0.06179\n",
      "Epochs: 7/40 Iteration: 1680 | Train loss: 0.06437\n",
      "Epochs: 7/40 Iteration: 1700 | Train loss: 0.07439\n",
      "Epochs: 7/40 Iteration: 1720 | Train loss: 0.03928\n",
      "Epochs: 7/40 Iteration: 1740 | Train loss: 0.08768\n",
      "Epochs: 8/40 Iteration: 1760 | Train loss: 0.05020\n",
      "Epochs: 8/40 Iteration: 1780 | Train loss: 0.04003\n",
      "Epochs: 8/40 Iteration: 1800 | Train loss: 0.03972\n",
      "Epochs: 8/40 Iteration: 1820 | Train loss: 0.22002\n",
      "Epochs: 8/40 Iteration: 1840 | Train loss: 0.06754\n",
      "Epochs: 8/40 Iteration: 1860 | Train loss: 0.04636\n",
      "Epochs: 8/40 Iteration: 1880 | Train loss: 0.14885\n",
      "Epochs: 8/40 Iteration: 1900 | Train loss: 0.07327\n",
      "Epochs: 8/40 Iteration: 1920 | Train loss: 0.08774\n",
      "Epochs: 8/40 Iteration: 1940 | Train loss: 0.04703\n",
      "Epochs: 8/40 Iteration: 1960 | Train loss: 0.09931\n",
      "Epochs: 8/40 Iteration: 1980 | Train loss: 0.10029\n",
      "Epochs: 8/40 Iteration: 2000 | Train loss: 0.04601\n",
      "Epochs: 9/40 Iteration: 2020 | Train loss: 0.06198\n",
      "Epochs: 9/40 Iteration: 2040 | Train loss: 0.00926\n",
      "Epochs: 9/40 Iteration: 2060 | Train loss: 0.03983\n",
      "Epochs: 9/40 Iteration: 2080 | Train loss: 0.05901\n",
      "Epochs: 9/40 Iteration: 2100 | Train loss: 0.01585\n",
      "Epochs: 9/40 Iteration: 2120 | Train loss: 0.06599\n",
      "Epochs: 9/40 Iteration: 2140 | Train loss: 0.00350\n",
      "Epochs: 9/40 Iteration: 2160 | Train loss: 0.01006\n",
      "Epochs: 9/40 Iteration: 2180 | Train loss: 0.01092\n",
      "Epochs: 9/40 Iteration: 2200 | Train loss: 0.01583\n",
      "Epochs: 9/40 Iteration: 2220 | Train loss: 0.04863\n",
      "Epochs: 9/40 Iteration: 2240 | Train loss: 0.04223\n",
      "Epochs: 10/40 Iteration: 2260 | Train loss: 0.02369\n",
      "Epochs: 10/40 Iteration: 2280 | Train loss: 0.10899\n",
      "Epochs: 10/40 Iteration: 2300 | Train loss: 0.00977\n",
      "Epochs: 10/40 Iteration: 2320 | Train loss: 0.00815\n",
      "Epochs: 10/40 Iteration: 2340 | Train loss: 0.01511\n",
      "Epochs: 10/40 Iteration: 2360 | Train loss: 0.01137\n",
      "Epochs: 10/40 Iteration: 2380 | Train loss: 0.04732\n",
      "Epochs: 10/40 Iteration: 2400 | Train loss: 0.05153\n",
      "Epochs: 10/40 Iteration: 2420 | Train loss: 0.06667\n",
      "Epochs: 10/40 Iteration: 2440 | Train loss: 0.08822\n",
      "Epochs: 10/40 Iteration: 2460 | Train loss: 0.02338\n",
      "Epochs: 10/40 Iteration: 2480 | Train loss: 0.04671\n",
      "Epochs: 10/40 Iteration: 2500 | Train loss: 0.04532\n",
      "Epochs: 11/40 Iteration: 2520 | Train loss: 0.09814\n",
      "Epochs: 11/40 Iteration: 2540 | Train loss: 0.02388\n",
      "Epochs: 11/40 Iteration: 2560 | Train loss: 0.00980\n",
      "Epochs: 11/40 Iteration: 2580 | Train loss: 0.04000\n",
      "Epochs: 11/40 Iteration: 2600 | Train loss: 0.05775\n",
      "Epochs: 11/40 Iteration: 2620 | Train loss: 0.04950\n",
      "Epochs: 11/40 Iteration: 2640 | Train loss: 0.00576\n",
      "Epochs: 11/40 Iteration: 2660 | Train loss: 0.01731\n",
      "Epochs: 11/40 Iteration: 2680 | Train loss: 0.09701\n",
      "Epochs: 11/40 Iteration: 2700 | Train loss: 0.00339\n",
      "Epochs: 11/40 Iteration: 2720 | Train loss: 0.00557\n",
      "Epochs: 11/40 Iteration: 2740 | Train loss: 0.06299\n",
      "Epochs: 12/40 Iteration: 2760 | Train loss: 0.17610\n",
      "Epochs: 12/40 Iteration: 2780 | Train loss: 0.04266\n",
      "Epochs: 12/40 Iteration: 2800 | Train loss: 0.01229\n",
      "Epochs: 12/40 Iteration: 2820 | Train loss: 0.00870\n",
      "Epochs: 12/40 Iteration: 2840 | Train loss: 0.00536\n",
      "Epochs: 12/40 Iteration: 2860 | Train loss: 0.01969\n",
      "Epochs: 12/40 Iteration: 2880 | Train loss: 0.02355\n",
      "Epochs: 12/40 Iteration: 2900 | Train loss: 0.00727\n",
      "Epochs: 12/40 Iteration: 2920 | Train loss: 0.04306\n",
      "Epochs: 12/40 Iteration: 2940 | Train loss: 0.00620\n",
      "Epochs: 12/40 Iteration: 2960 | Train loss: 0.00675\n",
      "Epochs: 12/40 Iteration: 2980 | Train loss: 0.00606\n",
      "Epochs: 12/40 Iteration: 3000 | Train loss: 0.03090\n",
      "Epochs: 13/40 Iteration: 3020 | Train loss: 0.02728\n",
      "Epochs: 13/40 Iteration: 3040 | Train loss: 0.01751\n",
      "Epochs: 13/40 Iteration: 3060 | Train loss: 0.00436\n",
      "Epochs: 13/40 Iteration: 3080 | Train loss: 0.03059\n",
      "Epochs: 13/40 Iteration: 3100 | Train loss: 0.00716\n",
      "Epochs: 13/40 Iteration: 3120 | Train loss: 0.05670\n",
      "Epochs: 13/40 Iteration: 3140 | Train loss: 0.00071\n",
      "Epochs: 13/40 Iteration: 3160 | Train loss: 0.00699\n",
      "Epochs: 13/40 Iteration: 3180 | Train loss: 0.00214\n",
      "Epochs: 13/40 Iteration: 3200 | Train loss: 0.00611\n",
      "Epochs: 13/40 Iteration: 3220 | Train loss: 0.01225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13/40 Iteration: 3240 | Train loss: 0.03627\n",
      "Epochs: 14/40 Iteration: 3260 | Train loss: 0.00709\n",
      "Epochs: 14/40 Iteration: 3280 | Train loss: 0.00530\n",
      "Epochs: 14/40 Iteration: 3300 | Train loss: 0.04619\n",
      "Epochs: 14/40 Iteration: 3320 | Train loss: 0.00383\n",
      "Epochs: 14/40 Iteration: 3340 | Train loss: 0.00228\n",
      "Epochs: 14/40 Iteration: 3360 | Train loss: 0.00376\n",
      "Epochs: 14/40 Iteration: 3380 | Train loss: 0.00852\n",
      "Epochs: 14/40 Iteration: 3400 | Train loss: 0.00202\n",
      "Epochs: 14/40 Iteration: 3420 | Train loss: 0.03039\n",
      "Epochs: 14/40 Iteration: 3440 | Train loss: 0.00295\n",
      "Epochs: 14/40 Iteration: 3460 | Train loss: 0.00215\n",
      "Epochs: 14/40 Iteration: 3480 | Train loss: 0.00254\n",
      "Epochs: 14/40 Iteration: 3500 | Train loss: 0.00554\n",
      "Epochs: 15/40 Iteration: 3520 | Train loss: 0.01981\n",
      "Epochs: 15/40 Iteration: 3540 | Train loss: 0.00087\n",
      "Epochs: 15/40 Iteration: 3560 | Train loss: 0.00387\n",
      "Epochs: 15/40 Iteration: 3580 | Train loss: 0.00126\n",
      "Epochs: 15/40 Iteration: 3600 | Train loss: 0.00632\n",
      "Epochs: 15/40 Iteration: 3620 | Train loss: 0.02675\n",
      "Epochs: 15/40 Iteration: 3640 | Train loss: 0.00250\n",
      "Epochs: 15/40 Iteration: 3660 | Train loss: 0.00288\n",
      "Epochs: 15/40 Iteration: 3680 | Train loss: 0.00081\n",
      "Epochs: 15/40 Iteration: 3700 | Train loss: 0.00552\n",
      "Epochs: 15/40 Iteration: 3720 | Train loss: 0.00019\n",
      "Epochs: 15/40 Iteration: 3740 | Train loss: 0.00281\n",
      "Epochs: 16/40 Iteration: 3760 | Train loss: 0.00055\n",
      "Epochs: 16/40 Iteration: 3780 | Train loss: 0.00041\n",
      "Epochs: 16/40 Iteration: 3800 | Train loss: 0.00114\n",
      "Epochs: 16/40 Iteration: 3820 | Train loss: 0.00289\n",
      "Epochs: 16/40 Iteration: 3840 | Train loss: 0.00061\n",
      "Epochs: 16/40 Iteration: 3860 | Train loss: 0.00452\n",
      "Epochs: 16/40 Iteration: 3880 | Train loss: 0.00233\n",
      "Epochs: 16/40 Iteration: 3900 | Train loss: 0.00136\n",
      "Epochs: 16/40 Iteration: 3920 | Train loss: 0.01658\n",
      "Epochs: 16/40 Iteration: 3940 | Train loss: 0.00440\n",
      "Epochs: 16/40 Iteration: 3960 | Train loss: 0.04085\n",
      "Epochs: 16/40 Iteration: 3980 | Train loss: 0.04066\n",
      "Epochs: 16/40 Iteration: 4000 | Train loss: 0.02878\n",
      "Epochs: 17/40 Iteration: 4020 | Train loss: 0.00328\n",
      "Epochs: 17/40 Iteration: 4040 | Train loss: 0.00137\n",
      "Epochs: 17/40 Iteration: 4060 | Train loss: 0.01232\n",
      "Epochs: 17/40 Iteration: 4080 | Train loss: 0.00900\n",
      "Epochs: 17/40 Iteration: 4100 | Train loss: 0.00480\n",
      "Epochs: 17/40 Iteration: 4120 | Train loss: 0.04531\n",
      "Epochs: 17/40 Iteration: 4140 | Train loss: 0.00184\n",
      "Epochs: 17/40 Iteration: 4160 | Train loss: 0.06609\n",
      "Epochs: 17/40 Iteration: 4180 | Train loss: 0.00361\n",
      "Epochs: 17/40 Iteration: 4200 | Train loss: 0.00091\n",
      "Epochs: 17/40 Iteration: 4220 | Train loss: 0.00218\n",
      "Epochs: 17/40 Iteration: 4240 | Train loss: 0.04532\n",
      "Epochs: 18/40 Iteration: 4260 | Train loss: 0.00433\n",
      "Epochs: 18/40 Iteration: 4280 | Train loss: 0.00073\n",
      "Epochs: 18/40 Iteration: 4300 | Train loss: 0.00383\n",
      "Epochs: 18/40 Iteration: 4320 | Train loss: 0.00285\n",
      "Epochs: 18/40 Iteration: 4340 | Train loss: 0.00193\n",
      "Epochs: 18/40 Iteration: 4360 | Train loss: 0.01558\n",
      "Epochs: 18/40 Iteration: 4380 | Train loss: 0.00847\n",
      "Epochs: 18/40 Iteration: 4400 | Train loss: 0.00095\n",
      "Epochs: 18/40 Iteration: 4420 | Train loss: 0.07130\n",
      "Epochs: 18/40 Iteration: 4440 | Train loss: 0.00400\n",
      "Epochs: 18/40 Iteration: 4460 | Train loss: 0.07212\n",
      "Epochs: 18/40 Iteration: 4480 | Train loss: 0.04257\n",
      "Epochs: 18/40 Iteration: 4500 | Train loss: 0.00209\n",
      "Epochs: 19/40 Iteration: 4520 | Train loss: 0.00900\n",
      "Epochs: 19/40 Iteration: 4540 | Train loss: 0.00660\n",
      "Epochs: 19/40 Iteration: 4560 | Train loss: 0.00910\n",
      "Epochs: 19/40 Iteration: 4580 | Train loss: 0.00085\n",
      "Epochs: 19/40 Iteration: 4600 | Train loss: 0.00525\n",
      "Epochs: 19/40 Iteration: 4620 | Train loss: 0.02021\n",
      "Epochs: 19/40 Iteration: 4640 | Train loss: 0.00873\n",
      "Epochs: 19/40 Iteration: 4660 | Train loss: 0.06403\n",
      "Epochs: 19/40 Iteration: 4680 | Train loss: 0.00245\n",
      "Epochs: 19/40 Iteration: 4700 | Train loss: 0.00915\n",
      "Epochs: 19/40 Iteration: 4720 | Train loss: 0.00061\n",
      "Epochs: 19/40 Iteration: 4740 | Train loss: 0.00368\n",
      "Epochs: 20/40 Iteration: 4760 | Train loss: 0.02184\n",
      "Epochs: 20/40 Iteration: 4780 | Train loss: 0.00055\n",
      "Epochs: 20/40 Iteration: 4800 | Train loss: 0.00145\n",
      "Epochs: 20/40 Iteration: 4820 | Train loss: 0.00050\n",
      "Epochs: 20/40 Iteration: 4840 | Train loss: 0.00107\n",
      "Epochs: 20/40 Iteration: 4860 | Train loss: 0.00076\n",
      "Epochs: 20/40 Iteration: 4880 | Train loss: 0.00295\n",
      "Epochs: 20/40 Iteration: 4900 | Train loss: 0.00099\n",
      "Epochs: 20/40 Iteration: 4920 | Train loss: 0.01029\n",
      "Epochs: 20/40 Iteration: 4940 | Train loss: 0.01320\n",
      "Epochs: 20/40 Iteration: 4960 | Train loss: 0.00139\n",
      "Epochs: 20/40 Iteration: 4980 | Train loss: 0.00240\n",
      "Epochs: 20/40 Iteration: 5000 | Train loss: 0.00512\n",
      "Epochs: 21/40 Iteration: 5020 | Train loss: 0.00067\n",
      "Epochs: 21/40 Iteration: 5040 | Train loss: 0.00033\n",
      "Epochs: 21/40 Iteration: 5060 | Train loss: 0.00279\n",
      "Epochs: 21/40 Iteration: 5080 | Train loss: 0.00082\n",
      "Epochs: 21/40 Iteration: 5100 | Train loss: 0.00678\n",
      "Epochs: 21/40 Iteration: 5120 | Train loss: 0.06722\n",
      "Epochs: 21/40 Iteration: 5140 | Train loss: 0.00238\n",
      "Epochs: 21/40 Iteration: 5160 | Train loss: 0.00324\n",
      "Epochs: 21/40 Iteration: 5180 | Train loss: 0.01069\n",
      "Epochs: 21/40 Iteration: 5200 | Train loss: 0.00032\n",
      "Epochs: 21/40 Iteration: 5220 | Train loss: 0.00031\n",
      "Epochs: 21/40 Iteration: 5240 | Train loss: 0.00111\n",
      "Epochs: 22/40 Iteration: 5260 | Train loss: 0.00031\n",
      "Epochs: 22/40 Iteration: 5280 | Train loss: 0.00089\n",
      "Epochs: 22/40 Iteration: 5300 | Train loss: 0.00872\n",
      "Epochs: 22/40 Iteration: 5320 | Train loss: 0.00595\n",
      "Epochs: 22/40 Iteration: 5340 | Train loss: 0.00118\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-87206b1a98ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-2ff290525899>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, y_train, num_epochs)\u001b[0m\n\u001b[1;32m    109\u001b[0m                     loss, _, state = sess.run(\n\u001b[1;32m    110\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0;34m'cost:0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_op'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                         feed_dict=feed)\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn.train(X_train, y_train, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ch16-model/sentiment-19.ckpt\n",
      "Test Acc.: 0.847\n"
     ]
    }
   ],
   "source": [
    "preds = rnn.predict(X_test)\n",
    "y_true = y_test[:len(preds)]\n",
    "\n",
    "print('Test Acc.: %.3f' % (np.sum(preds == y_true) / len(y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ch16-model/sentiment-19.ckpt\n",
      "[5.3644180e-07 9.9999857e-01 9.6604210e-01 ... 1.5084714e-06 4.7006497e-06\n",
      " 9.9465466e-01]\n"
     ]
    }
   ],
   "source": [
    "proba = rnn.predict(X_test, return_proba=True)\n",
    "print(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('../../../ch16_work/pg2265.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text[15858:]\n",
    "chars = set(text)\n",
    "\n",
    "char2int = {ch:i for i, ch in enumerate(chars)}\n",
    "int2char = dict(enumerate(chars))\n",
    "text_ints = np.array([char2int[ch] for ch in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163239\n"
     ]
    }
   ],
   "source": [
    "print(len(text_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "長文を一気に流し込むのではなく\n",
    "モデルの構造に合わせて適切に分割をして、トレーニングデータを作成する。\n",
    "\n",
    "なぜ？\n",
    "　　ミニバッチ単位で処理すると、行列で一気に演算できるので、学習時の演算効率が良い？\n",
    "    　　→　であればバッチサイズ単位で小分けすべき。\n",
    "      　　　　　ただ、それらの行データ間の系列は考慮できなくなるが、今回の場合は大丈夫なんだろう。\n",
    "　　あまりに巨大なステップ数だと、誤差が伝播しない？\n",
    "  　　→　であればステップ数をある程度の大きさに抑えるべき。\n",
    "\n",
    "LSTMのステップ数、LSTMに投入するバッチサイズを考慮して分割する。\n",
    "\n",
    "※用語\n",
    "バッチサイズ：　１バッチあたりのデータ件数（行数）\n",
    "バッチの個数：バッチサイズ/1個のバッチが何個あるか\n",
    "\n",
    "たとえば、\n",
    "\n",
    "【変換前の文字シーケンス】\n",
    "あいうえおかきくけこ\n",
    "さしすせそたちつてと\n",
    "なにぬねのはひふへほ\n",
    "まみむめもらりるれろい\n",
    "（文字数：41）\n",
    "\n",
    "【ステップ数】：5\n",
    "【バッチサイズ】：2\n",
    "\n",
    "とすると、\n",
    "\n",
    "【x】\n",
    "あいうえお　　　　　かきくけこ　　　　　さしすせそ　　　　　たちつてと\n",
    "なにぬねの　　　はひふへほ　　　まみむめも　　　　　らりるれろ\n",
    "\n",
    "【y】\n",
    "いうえおか　　　　　きくけこさ　　　　　しすせそた　　　　　ちつてとな\n",
    "にぬねのは　　　ひふへほま　　　みむめもら　　　　　りるれろい\n",
    "\n",
    "\n",
    "という感じに分割する。\n",
    "\n",
    "この場合のバッチの個数は4。\n",
    "'''\n",
    "def reshape_data(sequence, batch_size, num_steps):\n",
    "    # 1バッチで扱う文字数\n",
    "    mini_batch_length = batch_size * num_steps\n",
    "    \n",
    "    num_batches = int(len(sequence) / mini_batch_length)\n",
    "    \n",
    "    # 全文字数が1バッチで扱う文字数で、ちょうど割り切れる場合、\n",
    "    # xの最後の文字に対するyが無くなってしまう。そこで、バッチの数を一つ減らす。\n",
    "    if num_batches*mini_batch_length + 1 > len(sequence):\n",
    "        num_batches = num_batches - 1\n",
    "    \n",
    "    x = sequence[0: num_batches * mini_batch_length]\n",
    "    y = sequence[1: num_batches * mini_batch_length + 1]\n",
    "    \n",
    "    x_batch_splits = np.split(x, batch_size)\n",
    "    y_batch_splits = np.split(y, batch_size)\n",
    "    \n",
    "    x = np.stack(x_batch_splits)\n",
    "    y = np.stack(y_batch_splits)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def create_batch_generator(data_x, data_y, num_steps):\n",
    "    batch_size, tot_batch_length = data_x.shape\n",
    "    num_batches  = int(tot_batch_length / num_steps)\n",
    "    for b in range(num_batches):\n",
    "        yield (data_x[:, b*num_steps : (b+1)*num_steps],\n",
    "                  data_y[:, b*num_steps : (b+1)*num_steps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  (2, 20)\n",
      "y.shape:  (2, 20)\n",
      "x: \n",
      " [['あ' 'い' 'う' 'え' 'お' 'か' 'き' 'く' 'け' 'こ' 'さ' 'し' 'す' 'せ' 'そ' 'た' 'ち' 'つ' 'て' 'と']\n",
      " ['な' 'に' 'ぬ' 'ね' 'の' 'は' 'ひ' 'ふ' 'へ' 'ほ' 'ま' 'み' 'む' 'め' 'も' 'ら' 'り' 'る' 'れ' 'ろ']]\n",
      "y: \n",
      " [['い' 'う' 'え' 'お' 'か' 'き' 'く' 'け' 'こ' 'さ' 'し' 'す' 'せ' 'そ' 'た' 'ち' 'つ' 'て' 'と' 'な']\n",
      " ['に' 'ぬ' 'ね' 'の' 'は' 'ひ' 'ふ' 'へ' 'ほ' 'ま' 'み' 'む' 'め' 'も' 'ら' 'り' 'る' 'れ' 'ろ' 'い']]\n"
     ]
    }
   ],
   "source": [
    "a = 'あいうえおかきくけこさしすせそたちつてとなにぬねのはひふへほまみむめもらりるれろい'\n",
    "a = np.array([char for char in a])\n",
    "\n",
    "x, y = reshape_data(a, batch_size=2, num_steps=5)\n",
    "\n",
    "print('x.shape: ', x.shape)\n",
    "print('y.shape: ', y.shape)\n",
    "print('x: \\n', x)\n",
    "print('y: \\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  ['あ' 'い' 'う' 'え' 'お']\n",
      "     ['な' 'に' 'ぬ' 'ね' 'の']\n",
      "y:  ['い' 'う' 'え' 'お' 'か']\n",
      "     ['に' 'ぬ' 'ね' 'の' 'は']\n",
      "\n",
      "x:  ['か' 'き' 'く' 'け' 'こ']\n",
      "     ['は' 'ひ' 'ふ' 'へ' 'ほ']\n",
      "y:  ['き' 'く' 'け' 'こ' 'さ']\n",
      "     ['ひ' 'ふ' 'へ' 'ほ' 'ま']\n",
      "\n",
      "x:  ['さ' 'し' 'す' 'せ' 'そ']\n",
      "     ['ま' 'み' 'む' 'め' 'も']\n",
      "y:  ['し' 'す' 'せ' 'そ' 'た']\n",
      "     ['み' 'む' 'め' 'も' 'ら']\n",
      "\n",
      "x:  ['た' 'ち' 'つ' 'て' 'と']\n",
      "     ['ら' 'り' 'る' 'れ' 'ろ']\n",
      "y:  ['ち' 'つ' 'て' 'と' 'な']\n",
      "     ['り' 'る' 'れ' 'ろ' 'い']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "こうなるはず。\n",
    "\n",
    "【x】\n",
    "あいうえお　　　　　かきくけこ　　　　　さしすせそ　　　　　たちつてと\n",
    "なにぬねの　　　はひふへほ　　　まみむめも　　　　　らりるれろ\n",
    "\n",
    "【y】\n",
    "いうえおか　　　　　きくけこさ　　　　　しすせそた　　　　　ちつてとな\n",
    "にぬねのは　　　ひふへほま　　　みむめもら　　　　　りるれろい\n",
    "'''\n",
    "for xxx, yyy in create_batch_generator(x, y, num_steps=5):\n",
    "    print('x: ', xxx[0])\n",
    "    print('    ', xxx[1])\n",
    "    print('y: ', yyy[0])\n",
    "    print('    ', yyy[1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "class CharRNN(object):\n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=100,\n",
    "                            lstm_size=128, num_layers=1, learning_rate=0.001,\n",
    "                            keep_prob=0.5, grad_clip=5, sampling=False):\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.grad_clip = grad_clip\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            \n",
    "            self.build(sampling=sampling)\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "    def build(self, sampling):\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "            num_steps = self.num_steps\n",
    "            \n",
    "        tf_x = tf.placeholder(tf.int32, \n",
    "                                             shape=[batch_size, num_steps],\n",
    "                                             name='tf_x')\n",
    "        \n",
    "        tf_y = tf.placeholder(tf.int32,\n",
    "                                             shape=[batch_size, num_steps],\n",
    "                                             name='tf_y')\n",
    "\n",
    "        tf_keepprob = tf.placeholder(tf.float32,\n",
    "                                                            name='tf_keepprob')\n",
    "        \n",
    "        x_onehot = tf.one_hot(tf_x, depth=self.num_classes)\n",
    "        y_onehot = tf.one_hot(tf_y, depth=self.num_classes)\n",
    "\n",
    "        print('  <<  x_onehot  >> ', x_onehot)\n",
    "        print('  <<  y_onehot  >> ', y_onehot)\n",
    "\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "                                                [tf.contrib.rnn.DropoutWrapper(\n",
    "                                                    tr.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "                                                    output_keep_prob=tf_keepprob)\n",
    "\n",
    "                                                for _ in range(self.num_layers)])\n",
    "        \n",
    "        self.initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        lstm_outputs, self.final_state = \\\n",
    "            tf.nn.dynamic_rnn(cells, x_onehot, initial_state=self.initial_state)\n",
    "        \n",
    "        print('  <<  lstm_outputs  >> ', lstm_outputs)\n",
    "        \n",
    "        '''\n",
    "        バッチを跨いだ全ステップの出力（lstm_size）を一覧化する感じ\n",
    "        \n",
    "        バッチ１件目の1ステップの出力\n",
    "        バッチ１件目の1ステップの出力\n",
    "        ・・・\n",
    "        バッチ１件目のnステップの出力\n",
    "        バッチ2件目の1ステップの出力\n",
    "        バッチ2件目の1ステップの出力\n",
    "        ・・・\n",
    "        バッチ2件目のnステップの出力\n",
    "        ・・・\n",
    "        バッチm件目のnステップの出力\n",
    "        '''\n",
    "        seq_output_reshaped = tf.reshape(lstm_outputs,\n",
    "                                                                        shape=[-1, self.lstm_size],\n",
    "                                                                        name='seq_output_reshaped')\n",
    "        \n",
    "        # 各ステップの出力（lstm_size）から、文字種数のユニットに全結合。\n",
    "        # 文字ごとの確度を算出（どの文字に該当するか）。\n",
    "        logits = tf.layers.dense(inputs=seq_output_reshaped,\n",
    "                                                units=self.num_classes,\n",
    "                                                activation=None,\n",
    "                                                name='logits')\n",
    "        \n",
    "        # softmaxで確率の形式に変換（総和が1になるように変換）\n",
    "        proba = tf.nn.softmax(logits, name='probabilities')\n",
    "        \n",
    "        y_reshaped = tf.reshape(y_onehot,\n",
    "                                                   shape=[-1, self.num_classes])\n",
    "        \n",
    "        cost = tf.reduce_mean(\n",
    "            tf.nn.sofmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                                                      labels=y_reshaped),\n",
    "            name='cost')\n",
    "        \n",
    "        # 勾配刈り込み（まったく分からん）\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), \n",
    "                                                                     self.grad_clip)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars), name='train_op')\n",
    "\n",
    "    # train_xは、reshape_data()で分割した段階のもの。\n",
    "    def train(self, train_x, train_y, num_epochs, ckpt_dir='./ch16-model2/'):\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.mkdir(ckpt_dir)\n",
    "        \n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            \n",
    "            n_batches = int(train_x.shape[1] / self.num_steps)\n",
    "            iterations = n_batches * num_epochs\n",
    "            for epoch in range(num_epochs):\n",
    "                \n",
    "                new_state = sess.run(self.initial_state)\n",
    "                loss = 0\n",
    "                \n",
    "                bgen = create_batch_generator(train_x, train_y, self.num_steps)\n",
    "                for b, (batch_x, batch_y) in enumerate(bgen, 1):\n",
    "                    iteration = epoch * n_batches + b\n",
    "                    \n",
    "                    feed = {'tf_x:0': batch_x, 'tf_y:0': batch_y,\n",
    "                                   'tf_keepprob:0': self.keep_prob,\n",
    "                                   self.initial_state : new_state}\n",
    "                    \n",
    "                    batch_cost, _, new_state = sess.run(\n",
    "                        ['cost:0', 'train_op', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "                    \n",
    "                    if iteratione % 10 == 0:\n",
    "                        print('Epoch %d/%d Iteration %d| Training loss: %.4f' % \n",
    "                                 (epoch + 1, num_epochs, iteratione, batch_cost))\n",
    "                        \n",
    "                self.saver.save(sess, os.path.join(ckpt_dir, 'language_modeling.ckpt'))\n",
    "            \n",
    "    def sample(selff, output_length, ckpt_dir, starter_seq=\"The \"):\n",
    "        observed_seq = [ch for ch in starter_seq]\n",
    "        \n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(sess, tf.train.latest_checkpoint(ckpt_dir))\n",
    "            \n",
    "            new_state = sess.run(self.initial_state)\n",
    "            for ch in starter_seq:\n",
    "                x = np.zeros((1, 1))\n",
    "                x[0, 0] = char2int[ch]\n",
    "                feed = {'tf_x:0': x, 'tf_keepprob:0': 1.0, \n",
    "                               self.initial_state: new_state}\n",
    "                \n",
    "                proba, new_state = sess.run(\n",
    "                    ['probabilities:0', self.final_state],\n",
    "                    feed_dict=feed)\n",
    "                \n",
    "                ch_id = get_top_char(proba, len(chars))\n",
    "                observed_seq.append(int2char[ch_id])\n",
    "            \n",
    "            for i in range(output_length):\n",
    "                x[0, 0] = ch_id\n",
    "                feed = {'tf_x:0': x, 'tf_keepprob:0': 1.0,\n",
    "                               self.initial_state: new_state}\n",
    "                proba, new_state = sess.run(\n",
    "                    ['probabilities:0', self.final_state],\n",
    "                    feed_dict=feed)\n",
    "                \n",
    "                ch_id = get_top_char(proba, len(chars))\n",
    "                observed_seq.append(int2char[ch_id])\n",
    "                \n",
    "        return ''.join(observed_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
