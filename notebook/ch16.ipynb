{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../../../movie_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words occurrences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:16:21\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Counting words occurrences')\n",
    "for i, review in enumerate(df['review']):\n",
    "    # あとで半角スペースで単語を区切れるように、\n",
    "    # 句読点などの周りに半角スペースを挿入。\n",
    "    text = ''.join([c if c not in punctuation else ' '+c+' '\n",
    "                               for c in review]).lower()\n",
    "    df.loc[i, 'review'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this movie is great ! '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"This movie is great!\"\n",
    "''.join([c if c not in punctuation else ' '+c+' ' for c in a]).lower()\n",
    "\n",
    "#for c in a:\n",
    "#    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map reviews to ints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '.', ',', 'and', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:04\n"
     ]
    }
   ],
   "source": [
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "print(word_counts[:5])\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
    "\n",
    "mapped_reviews = []\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Map reviews to ints')\n",
    "for review in df['review']:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 200\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = review_arr[-sequence_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n",
      "[0 0 0 4 5]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((5,), dtype=int)\n",
    "print(a)\n",
    "b = np.array([1, 2, 3, 4, 5])\n",
    "a[-2:] = b[-2:]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequences[:25000, :]\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "\n",
    "X_test = sequences[25000:, :]\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x) // batch_size  # 切り捨て除算\n",
    "    x = x[:n_batches * batch_size]  # バッチサイズで割り切れない「はみ出し」を除外\n",
    "    if y is not None:\n",
    "        y = y[:n_batches * batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii : ii + batch_size], y[ii : ii + batch_size]\n",
    "        else:\n",
    "            yield x[ii : ii + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class SentimentRNN(object):\n",
    "    def __init__(self, n_words, seq_len=200,\n",
    "                            lstm_size=256, num_layers=1, batch_size=64,\n",
    "                            learning_rate=0.0001, embed_size=200):\n",
    "        self.n_words  = n_words\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers # LSTMのセル数\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "    def build(self):\n",
    "        tf_x = tf.placeholder(tf.int32,\n",
    "                                             shape=(self.batch_size, self.seq_len),\n",
    "                                             name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.float32,\n",
    "                                             shape=(self.batch_size),\n",
    "                                             name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32,\n",
    "                                                            name='tf_keepprob')\n",
    "        \n",
    "        embedding = tf.Variable(tf.random_uniform((self.n_words, self.embed_size),\n",
    "                                                                                     minval=-1, maxval=1),\n",
    "                                                   name='embedding')\n",
    "        \n",
    "        embed_x = tf.nn.embedding_lookup(embedding, tf_x, \n",
    "                                                                        name='embeded_x')\n",
    "        \n",
    "        print('  << embed_x >>  ', embed_x)\n",
    "        \n",
    "        # ここでいうセルとは、RNNの層のことっぽい\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "            [tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "                output_keep_prob=tf_keepprob)\n",
    "             \n",
    "            for i in range(self.num_layers)])\n",
    "        \n",
    "        self.initial_state = cells.zero_state(self.batch_size, tf.float32)\n",
    "        print('  << initial state >>  ', self.initial_state)\n",
    "        \n",
    "        # 埋め込みデータ、RNNセル、セルの初期状態をもとに、LSTMのパイプラインを作成する。\n",
    "        lstm_outputs, self.final_state = \\\n",
    "            tf.nn.dynamic_rnn(cells, embed_x, initial_state=self.initial_state)\n",
    "        \n",
    "        # lstm_outputsは、各タイムステップにおける、隠れ層ユニットの出力。ユニット数だけ存在する。\n",
    "        # 形状は(batch_size, タイムステップ数, 隠れ層のユニット数（lstm_size）)\n",
    "        # \n",
    "        # 参考：　https://jp.mathworks.com/help/deeplearning/ug/long-short-term-memory-networks.html;jsessionid=72c069a296d252b44e0d114132f6\n",
    "        # 参考：　https://orizuru.io/blog/machine-learning/lstm/\n",
    "        print('\\n  << lstm_output   >>  ', lstm_outputs)\n",
    "        \n",
    "        # final_stateは、最後のタイムステップにおける、「隠れ層の出力」と「セル状態」がセットになったもの。\n",
    "        # 参考：　https://stackoverflow.com/questions/49969349/hidden-states-vs-final-state-returned-by-tensorflows-dynamic-rnn\n",
    "        print('\\n  << final state   >>  ', self.final_state)\n",
    "        \n",
    "        # final_stateを次のミニバッチの初期状態にする理由は、\n",
    "        # ミニバッチ間の同一インデックスのデータどうしが系列関係にあって、\n",
    "        # その関係も考慮に入れて学習させるため。\n",
    "        # だが、今回の場合はそういった関係性はなさそうなので、本来は不要そう。\n",
    "        \n",
    "        logits = tf.layers.dense(inputs=lstm_outputs[:, -1],\n",
    "                                                units=1, activation=None,\n",
    "                                                name='logits')\n",
    "        \n",
    "        print('\\n  << logits  >>  ', logits)\n",
    "        \n",
    "        # サイズが１の次元を削除する。２次元→１次元（ベクトル）になる。\n",
    "        logits = tf.squeeze(logits, name='logits_squeezed')\n",
    "        print('\\n  << logits squeezed  >>  ', logits)\n",
    "        \n",
    "        y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
    "        \n",
    "        predictions = {\n",
    "            'probabilities': y_proba,\n",
    "            'labels': tf.cast(tf.round(y_proba), tf.int32, name='labels')\n",
    "        }\n",
    "        print('\\n  << predictions   >>  ', predictions)\n",
    "        \n",
    "        cost = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_y, logits=logits),\n",
    "            name='cost')\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name='train_op')\n",
    "    \n",
    "    def train(self, X_train, y_train, num_epochs):\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            iteration = 1\n",
    "            for epoch in range(num_epochs):\n",
    "                state = sess.run(self.initial_state)\n",
    "                \n",
    "                for batch_x, batch_y in create_batch_generator(\n",
    "                        X_train, y_train, self.batch_size):\n",
    "                    \n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                                   'tf_y:0': batch_y,\n",
    "                                   'tf_keepprob:0': 0.5,\n",
    "                                   self.initial_state: state}\n",
    "                    \n",
    "                    loss, _, state = sess.run(\n",
    "                        ['cost:0', 'train_op', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "                    \n",
    "                    if iteration % 20 == 0:\n",
    "                        print('Epochs: %d/%d Iteration: %d | Train loss: %.5f'\n",
    "                                     % (epoch + 1, num_epochs, iteration, loss))\n",
    "                        \n",
    "                    iteration += 1\n",
    "                \n",
    "                if(epoch+1) % 10 == 0:\n",
    "                    self.saver.save(sess, \"ch16-model/sentiment-%d.ckpt\" % epoch)\n",
    "        \n",
    "    # 予測のときにもミニバッチ間で最終状態の受け渡しをしている。\n",
    "    # あくまでも学習させたいのは重みであって、状態を学習させたいわけではない。\n",
    "    # ミニバッチ間の同一インデックスのデータどうしに系列関係がある場合は、\n",
    "    # 学習した重みを活用しつつ、状態を共有しないと正しい出力が出ない、ということ。\n",
    "    def predict(self, X_data, return_proba=False):\n",
    "        preds = []\n",
    "        with tf.Session(graph = self.g) as sess:\n",
    "            self.saver.restore(sess, tf.train.latest_checkpoint('./ch16-model/'))\n",
    "            test_state = sess.run(self.initial_state)\n",
    "        \n",
    "            for ii, batch_x in enumerate(create_batch_generator(\n",
    "                                                                X_data, \n",
    "                                                                None, \n",
    "                                                                batch_size=self.batch_size),\n",
    "                                                             1):\n",
    "\n",
    "                feed = {'tf_x:0': batch_x, 'tf_keepprob:0': 1.0,\n",
    "                               self.initial_state: test_state}\n",
    "\n",
    "                if return_proba:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "                else:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['labels:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "\n",
    "                preds.append(pred)\n",
    "            \n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  << embed_x >>   Tensor(\"embeded_x/Identity:0\", shape=(100, 200, 256), dtype=float32)\n",
      "  << initial state >>   (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      "  << lstm_output   >>   Tensor(\"rnn/transpose_1:0\", shape=(100, 200, 128), dtype=float32)\n",
      "\n",
      "  << final state   >>   (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      "  << logits  >>   Tensor(\"logits/BiasAdd:0\", shape=(100, 1), dtype=float32)\n",
      "\n",
      "  << logits squeezed  >>   Tensor(\"logits_squeezed:0\", shape=(100,), dtype=float32)\n",
      "\n",
      "  << predictions   >>   {'probabilities': <tf.Tensor 'probabilities:0' shape=(100,) dtype=float32>, 'labels': <tf.Tensor 'labels:0' shape=(100,) dtype=int32>}\n"
     ]
    }
   ],
   "source": [
    "# 長さが200未満のシーケンスを0でパディングするために、+1している。\n",
    "# Undefinedみたいな語彙を１つ足してる感じだと思う。\n",
    "n_words = max(list(word_to_int.values())) + 1\n",
    "\n",
    "rnn = SentimentRNN(n_words=n_words,\n",
    "                                      seq_len=sequence_length,\n",
    "                                      embed_size=256,\n",
    "                                      lstm_size=128,\n",
    "                                      num_layers=1,\n",
    "                                      batch_size=100,\n",
    "                                      learning_rate=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102967"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1/40 Iteration: 20 | Train loss: 0.68144\n",
      "Epochs: 1/40 Iteration: 40 | Train loss: 0.62962\n",
      "Epochs: 1/40 Iteration: 60 | Train loss: 0.64887\n",
      "Epochs: 1/40 Iteration: 80 | Train loss: 0.62561\n",
      "Epochs: 1/40 Iteration: 100 | Train loss: 0.57090\n",
      "Epochs: 1/40 Iteration: 120 | Train loss: 0.53749\n",
      "Epochs: 1/40 Iteration: 140 | Train loss: 0.48158\n",
      "Epochs: 1/40 Iteration: 160 | Train loss: 0.45924\n",
      "Epochs: 1/40 Iteration: 180 | Train loss: 0.47754\n",
      "Epochs: 1/40 Iteration: 200 | Train loss: 0.44295\n",
      "Epochs: 1/40 Iteration: 220 | Train loss: 0.39990\n",
      "Epochs: 1/40 Iteration: 240 | Train loss: 0.41825\n",
      "Epochs: 2/40 Iteration: 260 | Train loss: 0.39764\n",
      "Epochs: 2/40 Iteration: 280 | Train loss: 0.41892\n",
      "Epochs: 2/40 Iteration: 300 | Train loss: 0.51972\n",
      "Epochs: 2/40 Iteration: 320 | Train loss: 0.36637\n",
      "Epochs: 2/40 Iteration: 340 | Train loss: 0.32235\n",
      "Epochs: 2/40 Iteration: 360 | Train loss: 0.22373\n",
      "Epochs: 2/40 Iteration: 380 | Train loss: 0.35304\n",
      "Epochs: 2/40 Iteration: 400 | Train loss: 0.33850\n",
      "Epochs: 2/40 Iteration: 420 | Train loss: 0.31675\n",
      "Epochs: 2/40 Iteration: 440 | Train loss: 0.34686\n",
      "Epochs: 2/40 Iteration: 460 | Train loss: 0.46058\n",
      "Epochs: 2/40 Iteration: 480 | Train loss: 0.43436\n",
      "Epochs: 2/40 Iteration: 500 | Train loss: 0.40827\n",
      "Epochs: 3/40 Iteration: 520 | Train loss: 0.33363\n",
      "Epochs: 3/40 Iteration: 540 | Train loss: 0.29428\n",
      "Epochs: 3/40 Iteration: 560 | Train loss: 0.35254\n",
      "Epochs: 3/40 Iteration: 580 | Train loss: 0.26032\n",
      "Epochs: 3/40 Iteration: 600 | Train loss: 0.25236\n",
      "Epochs: 3/40 Iteration: 620 | Train loss: 0.28599\n",
      "Epochs: 3/40 Iteration: 640 | Train loss: 0.26728\n",
      "Epochs: 3/40 Iteration: 660 | Train loss: 0.25481\n",
      "Epochs: 3/40 Iteration: 680 | Train loss: 0.30043\n",
      "Epochs: 3/40 Iteration: 700 | Train loss: 0.19362\n",
      "Epochs: 3/40 Iteration: 720 | Train loss: 0.10855\n",
      "Epochs: 3/40 Iteration: 740 | Train loss: 0.31522\n",
      "Epochs: 4/40 Iteration: 760 | Train loss: 0.23287\n",
      "Epochs: 4/40 Iteration: 780 | Train loss: 0.17688\n",
      "Epochs: 4/40 Iteration: 800 | Train loss: 0.24049\n",
      "Epochs: 4/40 Iteration: 820 | Train loss: 0.26041\n",
      "Epochs: 4/40 Iteration: 840 | Train loss: 0.16279\n",
      "Epochs: 4/40 Iteration: 860 | Train loss: 0.13861\n",
      "Epochs: 4/40 Iteration: 880 | Train loss: 0.23361\n",
      "Epochs: 4/40 Iteration: 900 | Train loss: 0.19373\n",
      "Epochs: 4/40 Iteration: 920 | Train loss: 0.14055\n",
      "Epochs: 4/40 Iteration: 940 | Train loss: 0.19509\n",
      "Epochs: 4/40 Iteration: 960 | Train loss: 0.25872\n",
      "Epochs: 4/40 Iteration: 980 | Train loss: 0.09738\n",
      "Epochs: 4/40 Iteration: 1000 | Train loss: 0.07308\n",
      "Epochs: 5/40 Iteration: 1020 | Train loss: 0.15828\n",
      "Epochs: 5/40 Iteration: 1040 | Train loss: 0.13354\n",
      "Epochs: 5/40 Iteration: 1060 | Train loss: 0.17385\n",
      "Epochs: 5/40 Iteration: 1080 | Train loss: 0.13984\n",
      "Epochs: 5/40 Iteration: 1100 | Train loss: 0.09049\n",
      "Epochs: 5/40 Iteration: 1120 | Train loss: 0.15089\n",
      "Epochs: 5/40 Iteration: 1140 | Train loss: 0.09362\n",
      "Epochs: 5/40 Iteration: 1160 | Train loss: 0.06185\n",
      "Epochs: 5/40 Iteration: 1180 | Train loss: 0.18015\n",
      "Epochs: 5/40 Iteration: 1200 | Train loss: 0.10360\n",
      "Epochs: 5/40 Iteration: 1220 | Train loss: 0.05685\n",
      "Epochs: 5/40 Iteration: 1240 | Train loss: 0.16852\n",
      "Epochs: 6/40 Iteration: 1260 | Train loss: 0.16729\n",
      "Epochs: 6/40 Iteration: 1280 | Train loss: 0.07806\n",
      "Epochs: 6/40 Iteration: 1300 | Train loss: 0.16032\n",
      "Epochs: 6/40 Iteration: 1320 | Train loss: 0.07452\n",
      "Epochs: 6/40 Iteration: 1340 | Train loss: 0.07441\n",
      "Epochs: 6/40 Iteration: 1360 | Train loss: 0.17976\n",
      "Epochs: 6/40 Iteration: 1380 | Train loss: 0.14780\n",
      "Epochs: 6/40 Iteration: 1400 | Train loss: 0.16247\n",
      "Epochs: 6/40 Iteration: 1420 | Train loss: 0.13915\n",
      "Epochs: 6/40 Iteration: 1440 | Train loss: 0.20900\n",
      "Epochs: 6/40 Iteration: 1460 | Train loss: 0.43385\n",
      "Epochs: 6/40 Iteration: 1480 | Train loss: 0.14482\n",
      "Epochs: 6/40 Iteration: 1500 | Train loss: 0.11927\n",
      "Epochs: 7/40 Iteration: 1520 | Train loss: 0.11996\n",
      "Epochs: 7/40 Iteration: 1540 | Train loss: 0.04298\n",
      "Epochs: 7/40 Iteration: 1560 | Train loss: 0.21984\n",
      "Epochs: 7/40 Iteration: 1580 | Train loss: 0.09610\n",
      "Epochs: 7/40 Iteration: 1600 | Train loss: 0.09196\n",
      "Epochs: 7/40 Iteration: 1620 | Train loss: 0.18932\n",
      "Epochs: 7/40 Iteration: 1640 | Train loss: 0.12906\n",
      "Epochs: 7/40 Iteration: 1660 | Train loss: 0.06179\n",
      "Epochs: 7/40 Iteration: 1680 | Train loss: 0.06437\n",
      "Epochs: 7/40 Iteration: 1700 | Train loss: 0.07439\n",
      "Epochs: 7/40 Iteration: 1720 | Train loss: 0.03928\n",
      "Epochs: 7/40 Iteration: 1740 | Train loss: 0.08768\n",
      "Epochs: 8/40 Iteration: 1760 | Train loss: 0.05020\n",
      "Epochs: 8/40 Iteration: 1780 | Train loss: 0.04003\n",
      "Epochs: 8/40 Iteration: 1800 | Train loss: 0.03972\n",
      "Epochs: 8/40 Iteration: 1820 | Train loss: 0.22002\n",
      "Epochs: 8/40 Iteration: 1840 | Train loss: 0.06754\n",
      "Epochs: 8/40 Iteration: 1860 | Train loss: 0.04636\n",
      "Epochs: 8/40 Iteration: 1880 | Train loss: 0.14885\n",
      "Epochs: 8/40 Iteration: 1900 | Train loss: 0.07327\n",
      "Epochs: 8/40 Iteration: 1920 | Train loss: 0.08774\n",
      "Epochs: 8/40 Iteration: 1940 | Train loss: 0.04703\n",
      "Epochs: 8/40 Iteration: 1960 | Train loss: 0.09931\n",
      "Epochs: 8/40 Iteration: 1980 | Train loss: 0.10029\n",
      "Epochs: 8/40 Iteration: 2000 | Train loss: 0.04601\n",
      "Epochs: 9/40 Iteration: 2020 | Train loss: 0.06198\n",
      "Epochs: 9/40 Iteration: 2040 | Train loss: 0.00926\n",
      "Epochs: 9/40 Iteration: 2060 | Train loss: 0.03983\n",
      "Epochs: 9/40 Iteration: 2080 | Train loss: 0.05901\n",
      "Epochs: 9/40 Iteration: 2100 | Train loss: 0.01585\n",
      "Epochs: 9/40 Iteration: 2120 | Train loss: 0.06599\n",
      "Epochs: 9/40 Iteration: 2140 | Train loss: 0.00350\n",
      "Epochs: 9/40 Iteration: 2160 | Train loss: 0.01006\n",
      "Epochs: 9/40 Iteration: 2180 | Train loss: 0.01092\n",
      "Epochs: 9/40 Iteration: 2200 | Train loss: 0.01583\n",
      "Epochs: 9/40 Iteration: 2220 | Train loss: 0.04863\n",
      "Epochs: 9/40 Iteration: 2240 | Train loss: 0.04223\n",
      "Epochs: 10/40 Iteration: 2260 | Train loss: 0.02369\n",
      "Epochs: 10/40 Iteration: 2280 | Train loss: 0.10899\n",
      "Epochs: 10/40 Iteration: 2300 | Train loss: 0.00977\n",
      "Epochs: 10/40 Iteration: 2320 | Train loss: 0.00815\n",
      "Epochs: 10/40 Iteration: 2340 | Train loss: 0.01511\n",
      "Epochs: 10/40 Iteration: 2360 | Train loss: 0.01137\n",
      "Epochs: 10/40 Iteration: 2380 | Train loss: 0.04732\n",
      "Epochs: 10/40 Iteration: 2400 | Train loss: 0.05153\n",
      "Epochs: 10/40 Iteration: 2420 | Train loss: 0.06667\n",
      "Epochs: 10/40 Iteration: 2440 | Train loss: 0.08822\n",
      "Epochs: 10/40 Iteration: 2460 | Train loss: 0.02338\n",
      "Epochs: 10/40 Iteration: 2480 | Train loss: 0.04671\n",
      "Epochs: 10/40 Iteration: 2500 | Train loss: 0.04532\n",
      "Epochs: 11/40 Iteration: 2520 | Train loss: 0.09814\n",
      "Epochs: 11/40 Iteration: 2540 | Train loss: 0.02388\n",
      "Epochs: 11/40 Iteration: 2560 | Train loss: 0.00980\n",
      "Epochs: 11/40 Iteration: 2580 | Train loss: 0.04000\n",
      "Epochs: 11/40 Iteration: 2600 | Train loss: 0.05775\n",
      "Epochs: 11/40 Iteration: 2620 | Train loss: 0.04950\n",
      "Epochs: 11/40 Iteration: 2640 | Train loss: 0.00576\n",
      "Epochs: 11/40 Iteration: 2660 | Train loss: 0.01731\n",
      "Epochs: 11/40 Iteration: 2680 | Train loss: 0.09701\n",
      "Epochs: 11/40 Iteration: 2700 | Train loss: 0.00339\n",
      "Epochs: 11/40 Iteration: 2720 | Train loss: 0.00557\n",
      "Epochs: 11/40 Iteration: 2740 | Train loss: 0.06299\n",
      "Epochs: 12/40 Iteration: 2760 | Train loss: 0.17610\n",
      "Epochs: 12/40 Iteration: 2780 | Train loss: 0.04266\n",
      "Epochs: 12/40 Iteration: 2800 | Train loss: 0.01229\n",
      "Epochs: 12/40 Iteration: 2820 | Train loss: 0.00870\n",
      "Epochs: 12/40 Iteration: 2840 | Train loss: 0.00536\n",
      "Epochs: 12/40 Iteration: 2860 | Train loss: 0.01969\n",
      "Epochs: 12/40 Iteration: 2880 | Train loss: 0.02355\n",
      "Epochs: 12/40 Iteration: 2900 | Train loss: 0.00727\n",
      "Epochs: 12/40 Iteration: 2920 | Train loss: 0.04306\n",
      "Epochs: 12/40 Iteration: 2940 | Train loss: 0.00620\n",
      "Epochs: 12/40 Iteration: 2960 | Train loss: 0.00675\n",
      "Epochs: 12/40 Iteration: 2980 | Train loss: 0.00606\n",
      "Epochs: 12/40 Iteration: 3000 | Train loss: 0.03090\n",
      "Epochs: 13/40 Iteration: 3020 | Train loss: 0.02728\n",
      "Epochs: 13/40 Iteration: 3040 | Train loss: 0.01751\n",
      "Epochs: 13/40 Iteration: 3060 | Train loss: 0.00436\n",
      "Epochs: 13/40 Iteration: 3080 | Train loss: 0.03059\n",
      "Epochs: 13/40 Iteration: 3100 | Train loss: 0.00716\n",
      "Epochs: 13/40 Iteration: 3120 | Train loss: 0.05670\n",
      "Epochs: 13/40 Iteration: 3140 | Train loss: 0.00071\n",
      "Epochs: 13/40 Iteration: 3160 | Train loss: 0.00699\n",
      "Epochs: 13/40 Iteration: 3180 | Train loss: 0.00214\n",
      "Epochs: 13/40 Iteration: 3200 | Train loss: 0.00611\n",
      "Epochs: 13/40 Iteration: 3220 | Train loss: 0.01225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13/40 Iteration: 3240 | Train loss: 0.03627\n",
      "Epochs: 14/40 Iteration: 3260 | Train loss: 0.00709\n",
      "Epochs: 14/40 Iteration: 3280 | Train loss: 0.00530\n",
      "Epochs: 14/40 Iteration: 3300 | Train loss: 0.04619\n",
      "Epochs: 14/40 Iteration: 3320 | Train loss: 0.00383\n",
      "Epochs: 14/40 Iteration: 3340 | Train loss: 0.00228\n",
      "Epochs: 14/40 Iteration: 3360 | Train loss: 0.00376\n",
      "Epochs: 14/40 Iteration: 3380 | Train loss: 0.00852\n",
      "Epochs: 14/40 Iteration: 3400 | Train loss: 0.00202\n",
      "Epochs: 14/40 Iteration: 3420 | Train loss: 0.03039\n",
      "Epochs: 14/40 Iteration: 3440 | Train loss: 0.00295\n",
      "Epochs: 14/40 Iteration: 3460 | Train loss: 0.00215\n",
      "Epochs: 14/40 Iteration: 3480 | Train loss: 0.00254\n",
      "Epochs: 14/40 Iteration: 3500 | Train loss: 0.00554\n",
      "Epochs: 15/40 Iteration: 3520 | Train loss: 0.01981\n",
      "Epochs: 15/40 Iteration: 3540 | Train loss: 0.00087\n",
      "Epochs: 15/40 Iteration: 3560 | Train loss: 0.00387\n",
      "Epochs: 15/40 Iteration: 3580 | Train loss: 0.00126\n",
      "Epochs: 15/40 Iteration: 3600 | Train loss: 0.00632\n",
      "Epochs: 15/40 Iteration: 3620 | Train loss: 0.02675\n",
      "Epochs: 15/40 Iteration: 3640 | Train loss: 0.00250\n",
      "Epochs: 15/40 Iteration: 3660 | Train loss: 0.00288\n",
      "Epochs: 15/40 Iteration: 3680 | Train loss: 0.00081\n",
      "Epochs: 15/40 Iteration: 3700 | Train loss: 0.00552\n",
      "Epochs: 15/40 Iteration: 3720 | Train loss: 0.00019\n",
      "Epochs: 15/40 Iteration: 3740 | Train loss: 0.00281\n",
      "Epochs: 16/40 Iteration: 3760 | Train loss: 0.00055\n",
      "Epochs: 16/40 Iteration: 3780 | Train loss: 0.00041\n",
      "Epochs: 16/40 Iteration: 3800 | Train loss: 0.00114\n",
      "Epochs: 16/40 Iteration: 3820 | Train loss: 0.00289\n",
      "Epochs: 16/40 Iteration: 3840 | Train loss: 0.00061\n",
      "Epochs: 16/40 Iteration: 3860 | Train loss: 0.00452\n",
      "Epochs: 16/40 Iteration: 3880 | Train loss: 0.00233\n",
      "Epochs: 16/40 Iteration: 3900 | Train loss: 0.00136\n",
      "Epochs: 16/40 Iteration: 3920 | Train loss: 0.01658\n",
      "Epochs: 16/40 Iteration: 3940 | Train loss: 0.00440\n",
      "Epochs: 16/40 Iteration: 3960 | Train loss: 0.04085\n",
      "Epochs: 16/40 Iteration: 3980 | Train loss: 0.04066\n",
      "Epochs: 16/40 Iteration: 4000 | Train loss: 0.02878\n",
      "Epochs: 17/40 Iteration: 4020 | Train loss: 0.00328\n",
      "Epochs: 17/40 Iteration: 4040 | Train loss: 0.00137\n",
      "Epochs: 17/40 Iteration: 4060 | Train loss: 0.01232\n",
      "Epochs: 17/40 Iteration: 4080 | Train loss: 0.00900\n",
      "Epochs: 17/40 Iteration: 4100 | Train loss: 0.00480\n",
      "Epochs: 17/40 Iteration: 4120 | Train loss: 0.04531\n",
      "Epochs: 17/40 Iteration: 4140 | Train loss: 0.00184\n",
      "Epochs: 17/40 Iteration: 4160 | Train loss: 0.06609\n",
      "Epochs: 17/40 Iteration: 4180 | Train loss: 0.00361\n",
      "Epochs: 17/40 Iteration: 4200 | Train loss: 0.00091\n",
      "Epochs: 17/40 Iteration: 4220 | Train loss: 0.00218\n",
      "Epochs: 17/40 Iteration: 4240 | Train loss: 0.04532\n",
      "Epochs: 18/40 Iteration: 4260 | Train loss: 0.00433\n",
      "Epochs: 18/40 Iteration: 4280 | Train loss: 0.00073\n",
      "Epochs: 18/40 Iteration: 4300 | Train loss: 0.00383\n",
      "Epochs: 18/40 Iteration: 4320 | Train loss: 0.00285\n",
      "Epochs: 18/40 Iteration: 4340 | Train loss: 0.00193\n",
      "Epochs: 18/40 Iteration: 4360 | Train loss: 0.01558\n",
      "Epochs: 18/40 Iteration: 4380 | Train loss: 0.00847\n",
      "Epochs: 18/40 Iteration: 4400 | Train loss: 0.00095\n",
      "Epochs: 18/40 Iteration: 4420 | Train loss: 0.07130\n",
      "Epochs: 18/40 Iteration: 4440 | Train loss: 0.00400\n",
      "Epochs: 18/40 Iteration: 4460 | Train loss: 0.07212\n",
      "Epochs: 18/40 Iteration: 4480 | Train loss: 0.04257\n",
      "Epochs: 18/40 Iteration: 4500 | Train loss: 0.00209\n",
      "Epochs: 19/40 Iteration: 4520 | Train loss: 0.00900\n",
      "Epochs: 19/40 Iteration: 4540 | Train loss: 0.00660\n",
      "Epochs: 19/40 Iteration: 4560 | Train loss: 0.00910\n",
      "Epochs: 19/40 Iteration: 4580 | Train loss: 0.00085\n",
      "Epochs: 19/40 Iteration: 4600 | Train loss: 0.00525\n",
      "Epochs: 19/40 Iteration: 4620 | Train loss: 0.02021\n",
      "Epochs: 19/40 Iteration: 4640 | Train loss: 0.00873\n",
      "Epochs: 19/40 Iteration: 4660 | Train loss: 0.06403\n",
      "Epochs: 19/40 Iteration: 4680 | Train loss: 0.00245\n",
      "Epochs: 19/40 Iteration: 4700 | Train loss: 0.00915\n",
      "Epochs: 19/40 Iteration: 4720 | Train loss: 0.00061\n",
      "Epochs: 19/40 Iteration: 4740 | Train loss: 0.00368\n",
      "Epochs: 20/40 Iteration: 4760 | Train loss: 0.02184\n",
      "Epochs: 20/40 Iteration: 4780 | Train loss: 0.00055\n",
      "Epochs: 20/40 Iteration: 4800 | Train loss: 0.00145\n",
      "Epochs: 20/40 Iteration: 4820 | Train loss: 0.00050\n",
      "Epochs: 20/40 Iteration: 4840 | Train loss: 0.00107\n",
      "Epochs: 20/40 Iteration: 4860 | Train loss: 0.00076\n",
      "Epochs: 20/40 Iteration: 4880 | Train loss: 0.00295\n",
      "Epochs: 20/40 Iteration: 4900 | Train loss: 0.00099\n",
      "Epochs: 20/40 Iteration: 4920 | Train loss: 0.01029\n",
      "Epochs: 20/40 Iteration: 4940 | Train loss: 0.01320\n",
      "Epochs: 20/40 Iteration: 4960 | Train loss: 0.00139\n",
      "Epochs: 20/40 Iteration: 4980 | Train loss: 0.00240\n",
      "Epochs: 20/40 Iteration: 5000 | Train loss: 0.00512\n",
      "Epochs: 21/40 Iteration: 5020 | Train loss: 0.00067\n",
      "Epochs: 21/40 Iteration: 5040 | Train loss: 0.00033\n",
      "Epochs: 21/40 Iteration: 5060 | Train loss: 0.00279\n",
      "Epochs: 21/40 Iteration: 5080 | Train loss: 0.00082\n",
      "Epochs: 21/40 Iteration: 5100 | Train loss: 0.00678\n",
      "Epochs: 21/40 Iteration: 5120 | Train loss: 0.06722\n",
      "Epochs: 21/40 Iteration: 5140 | Train loss: 0.00238\n",
      "Epochs: 21/40 Iteration: 5160 | Train loss: 0.00324\n",
      "Epochs: 21/40 Iteration: 5180 | Train loss: 0.01069\n",
      "Epochs: 21/40 Iteration: 5200 | Train loss: 0.00032\n",
      "Epochs: 21/40 Iteration: 5220 | Train loss: 0.00031\n",
      "Epochs: 21/40 Iteration: 5240 | Train loss: 0.00111\n",
      "Epochs: 22/40 Iteration: 5260 | Train loss: 0.00031\n",
      "Epochs: 22/40 Iteration: 5280 | Train loss: 0.00089\n",
      "Epochs: 22/40 Iteration: 5300 | Train loss: 0.00872\n",
      "Epochs: 22/40 Iteration: 5320 | Train loss: 0.00595\n",
      "Epochs: 22/40 Iteration: 5340 | Train loss: 0.00118\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-87206b1a98ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-2ff290525899>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, y_train, num_epochs)\u001b[0m\n\u001b[1;32m    109\u001b[0m                     loss, _, state = sess.run(\n\u001b[1;32m    110\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0;34m'cost:0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_op'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                         feed_dict=feed)\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn.train(X_train, y_train, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ch16-model/sentiment-19.ckpt\n",
      "Test Acc.: 0.847\n"
     ]
    }
   ],
   "source": [
    "preds = rnn.predict(X_test)\n",
    "y_true = y_test[:len(preds)]\n",
    "\n",
    "print('Test Acc.: %.3f' % (np.sum(preds == y_true) / len(y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ch16-model/sentiment-19.ckpt\n",
      "[5.3644180e-07 9.9999857e-01 9.6604210e-01 ... 1.5084714e-06 4.7006497e-06\n",
      " 9.9465466e-01]\n"
     ]
    }
   ],
   "source": [
    "proba = rnn.predict(X_test, return_proba=True)\n",
    "print(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('../../../ch16_work/pg2265.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text[15858:]\n",
    "chars = set(text)\n",
    "\n",
    "char2int = {ch:i for i, ch in enumerate(chars)}\n",
    "int2char = dict(enumerate(chars))\n",
    "text_ints = np.array([char2int[ch] for ch in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163239\n"
     ]
    }
   ],
   "source": [
    "print(len(text_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "長文を一気に流し込むのではなく\n",
    "モデルの構造に合わせて適切に分割をして、トレーニングデータを作成する。\n",
    "\n",
    "なぜ？\n",
    "　　ミニバッチ単位で処理すると、行列で一気に演算できるので、学習時の演算効率が良い？\n",
    "    　　→　であればバッチサイズ単位で小分けすべき。\n",
    "      　　　　　ただ、それらの行データ間の系列は考慮できなくなるが、今回の場合は大丈夫なんだろう。\n",
    "　　あまりに巨大なステップ数だと、誤差が伝播しない？\n",
    "  　　→　であればステップ数をある程度の大きさに抑えるべき。\n",
    "\n",
    "LSTMのステップ数、LSTMに投入するバッチサイズを考慮して分割する。\n",
    "\n",
    "※用語\n",
    "バッチサイズ：　１バッチあたりのデータ件数（行数）\n",
    "バッチの個数：バッチサイズ/1個のバッチが何個あるか\n",
    "\n",
    "たとえば、\n",
    "\n",
    "【変換前の文字シーケンス】\n",
    "あいうえおかきくけこ\n",
    "さしすせそたちつてと\n",
    "なにぬねのはひふへほ\n",
    "まみむめもらりるれろい\n",
    "（文字数：41）\n",
    "\n",
    "【ステップ数】：5\n",
    "【バッチサイズ】：2\n",
    "\n",
    "とすると、\n",
    "\n",
    "【x】\n",
    "あいうえお　　　　　かきくけこ　　　　　さしすせそ　　　　　たちつてと\n",
    "なにぬねの　　　はひふへほ　　　まみむめも　　　　　らりるれろ\n",
    "\n",
    "【y】\n",
    "いうえおか　　　　　きくけこさ　　　　　しすせそた　　　　　ちつてとな\n",
    "にぬねのは　　　ひふへほま　　　みむめもら　　　　　りるれろい\n",
    "\n",
    "\n",
    "という感じに分割する。\n",
    "\n",
    "この場合のバッチの個数は4。\n",
    "'''\n",
    "def reshape_data(sequence, batch_size, num_steps):\n",
    "    # 1バッチで扱う文字数\n",
    "    mini_batch_length = batch_size * num_steps\n",
    "    \n",
    "    num_batches = int(len(sequence) / mini_batch_length)\n",
    "    \n",
    "    # 全文字数が1バッチで扱う文字数で、ちょうど割り切れる場合、\n",
    "    # xの最後の文字に対するyが無くなってしまう。そこで、バッチの数を一つ減らす。\n",
    "    if num_batches*mini_batch_length + 1 > len(sequence):\n",
    "        num_batches = num_batches - 1\n",
    "    \n",
    "    x = sequence[0: num_batches * mini_batch_length]\n",
    "    y = sequence[1: num_batches * mini_batch_length + 1]\n",
    "    \n",
    "    x_batch_splits = np.split(x, batch_size)\n",
    "    y_batch_splits = np.split(y, batch_size)\n",
    "    \n",
    "    x = np.stack(x_batch_splits)\n",
    "    y = np.stack(y_batch_splits)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def create_batch_generator(data_x, data_y, num_steps):\n",
    "    batch_size, tot_batch_length = data_x.shape\n",
    "    num_batches  = int(tot_batch_length / num_steps)\n",
    "    for b in range(num_batches):\n",
    "        yield (data_x[:, b*num_steps : (b+1)*num_steps],\n",
    "                  data_y[:, b*num_steps : (b+1)*num_steps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  (2, 20)\n",
      "y.shape:  (2, 20)\n",
      "x: \n",
      " [['あ' 'い' 'う' 'え' 'お' 'か' 'き' 'く' 'け' 'こ' 'さ' 'し' 'す' 'せ' 'そ' 'た' 'ち' 'つ' 'て' 'と']\n",
      " ['な' 'に' 'ぬ' 'ね' 'の' 'は' 'ひ' 'ふ' 'へ' 'ほ' 'ま' 'み' 'む' 'め' 'も' 'ら' 'り' 'る' 'れ' 'ろ']]\n",
      "y: \n",
      " [['い' 'う' 'え' 'お' 'か' 'き' 'く' 'け' 'こ' 'さ' 'し' 'す' 'せ' 'そ' 'た' 'ち' 'つ' 'て' 'と' 'な']\n",
      " ['に' 'ぬ' 'ね' 'の' 'は' 'ひ' 'ふ' 'へ' 'ほ' 'ま' 'み' 'む' 'め' 'も' 'ら' 'り' 'る' 'れ' 'ろ' 'い']]\n"
     ]
    }
   ],
   "source": [
    "a = 'あいうえおかきくけこさしすせそたちつてとなにぬねのはひふへほまみむめもらりるれろい'\n",
    "a = np.array([char for char in a])\n",
    "\n",
    "x, y = reshape_data(a, batch_size=2, num_steps=5)\n",
    "\n",
    "print('x.shape: ', x.shape)\n",
    "print('y.shape: ', y.shape)\n",
    "print('x: \\n', x)\n",
    "print('y: \\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  ['あ' 'い' 'う' 'え' 'お']\n",
      "     ['な' 'に' 'ぬ' 'ね' 'の']\n",
      "y:  ['い' 'う' 'え' 'お' 'か']\n",
      "     ['に' 'ぬ' 'ね' 'の' 'は']\n",
      "\n",
      "x:  ['か' 'き' 'く' 'け' 'こ']\n",
      "     ['は' 'ひ' 'ふ' 'へ' 'ほ']\n",
      "y:  ['き' 'く' 'け' 'こ' 'さ']\n",
      "     ['ひ' 'ふ' 'へ' 'ほ' 'ま']\n",
      "\n",
      "x:  ['さ' 'し' 'す' 'せ' 'そ']\n",
      "     ['ま' 'み' 'む' 'め' 'も']\n",
      "y:  ['し' 'す' 'せ' 'そ' 'た']\n",
      "     ['み' 'む' 'め' 'も' 'ら']\n",
      "\n",
      "x:  ['た' 'ち' 'つ' 'て' 'と']\n",
      "     ['ら' 'り' 'る' 'れ' 'ろ']\n",
      "y:  ['ち' 'つ' 'て' 'と' 'な']\n",
      "     ['り' 'る' 'れ' 'ろ' 'い']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "こうなるはず。\n",
    "\n",
    "【x】\n",
    "あいうえお　　　　　かきくけこ　　　　　さしすせそ　　　　　たちつてと\n",
    "なにぬねの　　　はひふへほ　　　まみむめも　　　　　らりるれろ\n",
    "\n",
    "【y】\n",
    "いうえおか　　　　　きくけこさ　　　　　しすせそた　　　　　ちつてとな\n",
    "にぬねのは　　　ひふへほま　　　みむめもら　　　　　りるれろい\n",
    "'''\n",
    "for xxx, yyy in create_batch_generator(x, y, num_steps=5):\n",
    "    print('x: ', xxx[0])\n",
    "    print('    ', xxx[1])\n",
    "    print('y: ', yyy[0])\n",
    "    print('    ', yyy[1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "class CharRNN(object):\n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=100,\n",
    "                            lstm_size=128, num_layers=1, learning_rate=0.001,\n",
    "                            keep_prob=0.5, grad_clip=5, sampling=False):\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.grad_clip = grad_clip\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            \n",
    "            self.build(sampling=sampling)\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "    def build(self, sampling):\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "            num_steps = self.num_steps\n",
    "            \n",
    "        tf_x = tf.placeholder(tf.int32, \n",
    "                                             shape=[batch_size, num_steps],\n",
    "                                             name='tf_x')\n",
    "        \n",
    "        tf_y = tf.placeholder(tf.int32,\n",
    "                                             shape=[batch_size, num_steps],\n",
    "                                             name='tf_y')\n",
    "\n",
    "        tf_keepprob = tf.placeholder(tf.float32,\n",
    "                                                            name='tf_keepprob')\n",
    "        \n",
    "        x_onehot = tf.one_hot(tf_x, depth=self.num_classes)\n",
    "        y_onehot = tf.one_hot(tf_y, depth=self.num_classes)\n",
    "\n",
    "        print('  <<  x_onehot  >> ', x_onehot)\n",
    "        print('  <<  y_onehot  >> ', y_onehot)\n",
    "\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "                                                [tf.contrib.rnn.DropoutWrapper(\n",
    "                                                    tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "                                                    output_keep_prob=tf_keepprob) for _ in range(self.num_layers)])\n",
    "        \n",
    "        self.initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        lstm_outputs, self.final_state = \\\n",
    "            tf.nn.dynamic_rnn(cells, x_onehot, initial_state=self.initial_state)\n",
    "        \n",
    "        print('  <<  lstm_outputs  >> ', lstm_outputs)\n",
    "        \n",
    "        '''\n",
    "        バッチを跨いだ全ステップの出力（lstm_size）を一覧化する感じ\n",
    "        \n",
    "        バッチ１件目の1ステップの出力\n",
    "        バッチ１件目の1ステップの出力\n",
    "        ・・・\n",
    "        バッチ１件目のnステップの出力\n",
    "        バッチ2件目の1ステップの出力\n",
    "        バッチ2件目の1ステップの出力\n",
    "        ・・・\n",
    "        バッチ2件目のnステップの出力\n",
    "        ・・・\n",
    "        バッチm件目のnステップの出力\n",
    "        '''\n",
    "        seq_output_reshaped = tf.reshape(lstm_outputs,\n",
    "                                                                        shape=[-1, self.lstm_size],\n",
    "                                                                        name='seq_output_reshaped')\n",
    "        \n",
    "        # 各ステップの出力（lstm_size）から、文字種数のユニットに全結合。\n",
    "        # 文字ごとの確度を算出（どの文字に該当するか）。\n",
    "        logits = tf.layers.dense(inputs=seq_output_reshaped,\n",
    "                                                units=self.num_classes,\n",
    "                                                activation=None,\n",
    "                                                name='logits')\n",
    "        \n",
    "        # softmaxで確率の形式に変換（総和が1になるように変換）\n",
    "        proba = tf.nn.softmax(logits, name='probabilities')\n",
    "        \n",
    "        y_reshaped = tf.reshape(y_onehot,\n",
    "                                                   shape=[-1, self.num_classes])\n",
    "        \n",
    "        cost = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                                                      labels=y_reshaped),\n",
    "            name='cost')\n",
    "        \n",
    "        # 勾配刈り込み（まったく分からん）\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), \n",
    "                                                                     self.grad_clip)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars), name='train_op')\n",
    "\n",
    "    # train_xは、reshape_data()で分割した段階のもの。\n",
    "    def train(self, train_x, train_y, num_epochs, ckpt_dir='./ch16-model2/'):\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.mkdir(ckpt_dir)\n",
    "        \n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            \n",
    "            n_batches = int(train_x.shape[1] / self.num_steps)\n",
    "            iterations = n_batches * num_epochs\n",
    "            for epoch in range(num_epochs):\n",
    "                \n",
    "                new_state = sess.run(self.initial_state)\n",
    "                loss = 0\n",
    "                \n",
    "                bgen = create_batch_generator(train_x, train_y, self.num_steps)\n",
    "                for b, (batch_x, batch_y) in enumerate(bgen, 1):\n",
    "                    iteration = epoch * n_batches + b\n",
    "                    \n",
    "                    feed = {'tf_x:0': batch_x, 'tf_y:0': batch_y,\n",
    "                                   'tf_keepprob:0': self.keep_prob,\n",
    "                                   self.initial_state : new_state}\n",
    "                    \n",
    "                    batch_cost, _, new_state = sess.run(\n",
    "                        ['cost:0', 'train_op', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "                    \n",
    "                    if iteration % 10 == 0:\n",
    "                        print('Epoch %d/%d Iteration %d| Training loss: %.4f' % \n",
    "                                 (epoch + 1, num_epochs, iteration, batch_cost))\n",
    "                        \n",
    "                self.saver.save(sess, os.path.join(ckpt_dir, 'language_modeling.ckpt'))\n",
    "            \n",
    "    def sample(self, output_length, ckpt_dir, starter_seq=\"The \"):\n",
    "        observed_seq = [ch for ch in starter_seq]\n",
    "        \n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(sess, tf.train.latest_checkpoint(ckpt_dir))\n",
    "            \n",
    "            # 1文字単位でモデルを実行し、\n",
    "            # 得られた最終状態を次の文字で実行する時に利用する。\n",
    "            # モデルのサンプリングモードは、このような形態のために用意されている。\n",
    "            new_state = sess.run(self.initial_state)\n",
    "            for ch in starter_seq:\n",
    "                x = np.zeros((1, 1))\n",
    "                x[0, 0] = char2int[ch]\n",
    "                feed = {'tf_x:0': x, 'tf_keepprob:0': 1.0, \n",
    "                               self.initial_state: new_state}\n",
    "                \n",
    "                proba, new_state = sess.run(\n",
    "                    ['probabilities:0', self.final_state],\n",
    "                    feed_dict=feed)\n",
    "                \n",
    "            ch_id = get_top_char(proba, len(chars))\n",
    "            observed_seq.append(int2char[ch_id])\n",
    "            \n",
    "            for i in range(output_length):\n",
    "                x[0, 0] = ch_id\n",
    "                feed = {'tf_x:0': x, 'tf_keepprob:0': 1.0,\n",
    "                               self.initial_state: new_state}\n",
    "                proba, new_state = sess.run(\n",
    "                    ['probabilities:0', self.final_state],\n",
    "                    feed_dict=feed)\n",
    "                \n",
    "                ch_id = get_top_char(proba, len(chars))\n",
    "                observed_seq.append(int2char[ch_id])\n",
    "                \n",
    "        return ''.join(observed_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_char(probas, char_size, top_n=5):\n",
    "    p = np.squeeze(probas)\n",
    "    \n",
    "    # 順位がtop_n未満の要素について、値（確率）をゼロにする\n",
    "    p[np.argsort(p)[:-top_n]] = 0.0\n",
    "    \n",
    "    # 順位がtop_nの要素間で、値（確率）を標準化（合計が1になるようにしてる？）\n",
    "    p = p / np.sum(p)\n",
    "    \n",
    "    # 0〜char_sizeの整数値から、pが表す確率に従って、ランダムに値を選択。\n",
    "    ch_id = np.random.choice(char_size, 1, p=p)[0]\n",
    "    \n",
    "    return ch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas = np.array([0.01, 0.01, 0.01, 0.9, 0.01, 0.01, 0.05, 0.0, 0.0, 0.0])\n",
    "get_top_char(probas, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  <<  x_onehot  >>  Tensor(\"one_hot:0\", shape=(64, 100, 68), dtype=float32)\n",
      "  <<  y_onehot  >>  Tensor(\"one_hot_1:0\", shape=(64, 100, 68), dtype=float32)\n",
      "  <<  lstm_outputs  >>  Tensor(\"rnn/transpose_1:0\", shape=(64, 100, 128), dtype=float32)\n",
      "Epoch 1/100 Iteration 10| Training loss: 3.7141\n",
      "Epoch 1/100 Iteration 20| Training loss: 3.3834\n",
      "Epoch 2/100 Iteration 30| Training loss: 3.3147\n",
      "Epoch 2/100 Iteration 40| Training loss: 3.2510\n",
      "Epoch 2/100 Iteration 50| Training loss: 3.2558\n",
      "Epoch 3/100 Iteration 60| Training loss: 3.2004\n",
      "Epoch 3/100 Iteration 70| Training loss: 3.2061\n",
      "Epoch 4/100 Iteration 80| Training loss: 3.1945\n",
      "Epoch 4/100 Iteration 90| Training loss: 3.1483\n",
      "Epoch 4/100 Iteration 100| Training loss: 3.1705\n",
      "Epoch 5/100 Iteration 110| Training loss: 3.1185\n",
      "Epoch 5/100 Iteration 120| Training loss: 3.1142\n",
      "Epoch 6/100 Iteration 130| Training loss: 3.0933\n",
      "Epoch 6/100 Iteration 140| Training loss: 3.0267\n",
      "Epoch 6/100 Iteration 150| Training loss: 3.0222\n",
      "Epoch 7/100 Iteration 160| Training loss: 2.9628\n",
      "Epoch 7/100 Iteration 170| Training loss: 2.9517\n",
      "Epoch 8/100 Iteration 180| Training loss: 2.9029\n",
      "Epoch 8/100 Iteration 190| Training loss: 2.8496\n",
      "Epoch 8/100 Iteration 200| Training loss: 2.8454\n",
      "Epoch 9/100 Iteration 210| Training loss: 2.7798\n",
      "Epoch 9/100 Iteration 220| Training loss: 2.7733\n",
      "Epoch 10/100 Iteration 230| Training loss: 2.7376\n",
      "Epoch 10/100 Iteration 240| Training loss: 2.6793\n",
      "Epoch 10/100 Iteration 250| Training loss: 2.6755\n",
      "Epoch 11/100 Iteration 260| Training loss: 2.6176\n",
      "Epoch 11/100 Iteration 270| Training loss: 2.6476\n",
      "Epoch 12/100 Iteration 280| Training loss: 2.6002\n",
      "Epoch 12/100 Iteration 290| Training loss: 2.5575\n",
      "Epoch 12/100 Iteration 300| Training loss: 2.5548\n",
      "Epoch 13/100 Iteration 310| Training loss: 2.5181\n",
      "Epoch 13/100 Iteration 320| Training loss: 2.5432\n",
      "Epoch 14/100 Iteration 330| Training loss: 2.4994\n",
      "Epoch 14/100 Iteration 340| Training loss: 2.4778\n",
      "Epoch 14/100 Iteration 350| Training loss: 2.4911\n",
      "Epoch 15/100 Iteration 360| Training loss: 2.4433\n",
      "Epoch 15/100 Iteration 370| Training loss: 2.4886\n",
      "Epoch 16/100 Iteration 380| Training loss: 2.4372\n",
      "Epoch 16/100 Iteration 390| Training loss: 2.4163\n",
      "Epoch 16/100 Iteration 400| Training loss: 2.4275\n",
      "Epoch 17/100 Iteration 410| Training loss: 2.3808\n",
      "Epoch 17/100 Iteration 420| Training loss: 2.4288\n",
      "Epoch 18/100 Iteration 430| Training loss: 2.3814\n",
      "Epoch 18/100 Iteration 440| Training loss: 2.3716\n",
      "Epoch 18/100 Iteration 450| Training loss: 2.3821\n",
      "Epoch 19/100 Iteration 460| Training loss: 2.3471\n",
      "Epoch 19/100 Iteration 470| Training loss: 2.3975\n",
      "Epoch 20/100 Iteration 480| Training loss: 2.3539\n",
      "Epoch 20/100 Iteration 490| Training loss: 2.3373\n",
      "Epoch 20/100 Iteration 500| Training loss: 2.3456\n",
      "Epoch 21/100 Iteration 510| Training loss: 2.3129\n",
      "Epoch 21/100 Iteration 520| Training loss: 2.3623\n",
      "Epoch 22/100 Iteration 530| Training loss: 2.3119\n",
      "Epoch 22/100 Iteration 540| Training loss: 2.2960\n",
      "Epoch 22/100 Iteration 550| Training loss: 2.3160\n",
      "Epoch 23/100 Iteration 560| Training loss: 2.2898\n",
      "Epoch 23/100 Iteration 570| Training loss: 2.3372\n",
      "Epoch 24/100 Iteration 580| Training loss: 2.2861\n",
      "Epoch 24/100 Iteration 590| Training loss: 2.2687\n",
      "Epoch 24/100 Iteration 600| Training loss: 2.2873\n",
      "Epoch 25/100 Iteration 610| Training loss: 2.2578\n",
      "Epoch 25/100 Iteration 620| Training loss: 2.3021\n",
      "Epoch 26/100 Iteration 630| Training loss: 2.2527\n",
      "Epoch 26/100 Iteration 640| Training loss: 2.2442\n",
      "Epoch 26/100 Iteration 650| Training loss: 2.2508\n",
      "Epoch 27/100 Iteration 660| Training loss: 2.2314\n",
      "Epoch 27/100 Iteration 670| Training loss: 2.2776\n",
      "Epoch 28/100 Iteration 680| Training loss: 2.2249\n",
      "Epoch 28/100 Iteration 690| Training loss: 2.2226\n",
      "Epoch 28/100 Iteration 700| Training loss: 2.2480\n",
      "Epoch 29/100 Iteration 710| Training loss: 2.2115\n",
      "Epoch 29/100 Iteration 720| Training loss: 2.2524\n",
      "Epoch 30/100 Iteration 730| Training loss: 2.2103\n",
      "Epoch 30/100 Iteration 740| Training loss: 2.2071\n",
      "Epoch 30/100 Iteration 750| Training loss: 2.2210\n",
      "Epoch 31/100 Iteration 760| Training loss: 2.1872\n",
      "Epoch 31/100 Iteration 770| Training loss: 2.2393\n",
      "Epoch 32/100 Iteration 780| Training loss: 2.1909\n",
      "Epoch 32/100 Iteration 790| Training loss: 2.1797\n",
      "Epoch 32/100 Iteration 800| Training loss: 2.2012\n",
      "Epoch 33/100 Iteration 810| Training loss: 2.1661\n",
      "Epoch 33/100 Iteration 820| Training loss: 2.2204\n",
      "Epoch 34/100 Iteration 830| Training loss: 2.1783\n",
      "Epoch 34/100 Iteration 840| Training loss: 2.1659\n",
      "Epoch 34/100 Iteration 850| Training loss: 2.1898\n",
      "Epoch 35/100 Iteration 860| Training loss: 2.1510\n",
      "Epoch 35/100 Iteration 870| Training loss: 2.2024\n",
      "Epoch 36/100 Iteration 880| Training loss: 2.1505\n",
      "Epoch 36/100 Iteration 890| Training loss: 2.1547\n",
      "Epoch 36/100 Iteration 900| Training loss: 2.1691\n",
      "Epoch 37/100 Iteration 910| Training loss: 2.1269\n",
      "Epoch 37/100 Iteration 920| Training loss: 2.1833\n",
      "Epoch 38/100 Iteration 930| Training loss: 2.1353\n",
      "Epoch 38/100 Iteration 940| Training loss: 2.1314\n",
      "Epoch 38/100 Iteration 950| Training loss: 2.1558\n",
      "Epoch 39/100 Iteration 960| Training loss: 2.1128\n",
      "Epoch 39/100 Iteration 970| Training loss: 2.1754\n",
      "Epoch 40/100 Iteration 980| Training loss: 2.1210\n",
      "Epoch 40/100 Iteration 990| Training loss: 2.1242\n",
      "Epoch 40/100 Iteration 1000| Training loss: 2.1390\n",
      "Epoch 41/100 Iteration 1010| Training loss: 2.0943\n",
      "Epoch 41/100 Iteration 1020| Training loss: 2.1472\n",
      "Epoch 42/100 Iteration 1030| Training loss: 2.1189\n",
      "Epoch 42/100 Iteration 1040| Training loss: 2.1072\n",
      "Epoch 42/100 Iteration 1050| Training loss: 2.1180\n",
      "Epoch 43/100 Iteration 1060| Training loss: 2.0951\n",
      "Epoch 43/100 Iteration 1070| Training loss: 2.1406\n",
      "Epoch 44/100 Iteration 1080| Training loss: 2.0949\n",
      "Epoch 44/100 Iteration 1090| Training loss: 2.0926\n",
      "Epoch 44/100 Iteration 1100| Training loss: 2.0960\n",
      "Epoch 45/100 Iteration 1110| Training loss: 2.0688\n",
      "Epoch 45/100 Iteration 1120| Training loss: 2.1287\n",
      "Epoch 46/100 Iteration 1130| Training loss: 2.0954\n",
      "Epoch 46/100 Iteration 1140| Training loss: 2.0771\n",
      "Epoch 46/100 Iteration 1150| Training loss: 2.0943\n",
      "Epoch 47/100 Iteration 1160| Training loss: 2.0560\n",
      "Epoch 47/100 Iteration 1170| Training loss: 2.1086\n",
      "Epoch 48/100 Iteration 1180| Training loss: 2.0693\n",
      "Epoch 48/100 Iteration 1190| Training loss: 2.0640\n",
      "Epoch 48/100 Iteration 1200| Training loss: 2.0780\n",
      "Epoch 49/100 Iteration 1210| Training loss: 2.0480\n",
      "Epoch 49/100 Iteration 1220| Training loss: 2.1021\n",
      "Epoch 50/100 Iteration 1230| Training loss: 2.0448\n",
      "Epoch 50/100 Iteration 1240| Training loss: 2.0608\n",
      "Epoch 50/100 Iteration 1250| Training loss: 2.0696\n",
      "Epoch 51/100 Iteration 1260| Training loss: 2.0444\n",
      "Epoch 51/100 Iteration 1270| Training loss: 2.0993\n",
      "Epoch 52/100 Iteration 1280| Training loss: 2.0438\n",
      "Epoch 52/100 Iteration 1290| Training loss: 2.0394\n",
      "Epoch 52/100 Iteration 1300| Training loss: 2.0480\n",
      "Epoch 53/100 Iteration 1310| Training loss: 2.0147\n",
      "Epoch 53/100 Iteration 1320| Training loss: 2.0777\n",
      "Epoch 54/100 Iteration 1330| Training loss: 2.0328\n",
      "Epoch 54/100 Iteration 1340| Training loss: 2.0290\n",
      "Epoch 54/100 Iteration 1350| Training loss: 2.0427\n",
      "Epoch 55/100 Iteration 1360| Training loss: 2.0065\n",
      "Epoch 55/100 Iteration 1370| Training loss: 2.0652\n",
      "Epoch 56/100 Iteration 1380| Training loss: 2.0326\n",
      "Epoch 56/100 Iteration 1390| Training loss: 2.0232\n",
      "Epoch 56/100 Iteration 1400| Training loss: 2.0324\n",
      "Epoch 57/100 Iteration 1410| Training loss: 1.9959\n",
      "Epoch 57/100 Iteration 1420| Training loss: 2.0596\n",
      "Epoch 58/100 Iteration 1430| Training loss: 2.0149\n",
      "Epoch 58/100 Iteration 1440| Training loss: 2.0225\n",
      "Epoch 58/100 Iteration 1450| Training loss: 2.0250\n",
      "Epoch 59/100 Iteration 1460| Training loss: 1.9901\n",
      "Epoch 59/100 Iteration 1470| Training loss: 2.0545\n",
      "Epoch 60/100 Iteration 1480| Training loss: 2.0100\n",
      "Epoch 60/100 Iteration 1490| Training loss: 1.9979\n",
      "Epoch 60/100 Iteration 1500| Training loss: 2.0184\n",
      "Epoch 61/100 Iteration 1510| Training loss: 1.9870\n",
      "Epoch 61/100 Iteration 1520| Training loss: 2.0463\n",
      "Epoch 62/100 Iteration 1530| Training loss: 1.9959\n",
      "Epoch 62/100 Iteration 1540| Training loss: 1.9990\n",
      "Epoch 62/100 Iteration 1550| Training loss: 2.0091\n",
      "Epoch 63/100 Iteration 1560| Training loss: 1.9646\n",
      "Epoch 63/100 Iteration 1570| Training loss: 2.0216\n",
      "Epoch 64/100 Iteration 1580| Training loss: 1.9957\n",
      "Epoch 64/100 Iteration 1590| Training loss: 1.9894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100 Iteration 1600| Training loss: 1.9915\n",
      "Epoch 65/100 Iteration 1610| Training loss: 1.9628\n",
      "Epoch 65/100 Iteration 1620| Training loss: 2.0120\n",
      "Epoch 66/100 Iteration 1630| Training loss: 1.9771\n",
      "Epoch 66/100 Iteration 1640| Training loss: 1.9799\n",
      "Epoch 66/100 Iteration 1650| Training loss: 1.9766\n",
      "Epoch 67/100 Iteration 1660| Training loss: 1.9459\n",
      "Epoch 67/100 Iteration 1670| Training loss: 1.9993\n",
      "Epoch 68/100 Iteration 1680| Training loss: 1.9738\n",
      "Epoch 68/100 Iteration 1690| Training loss: 1.9814\n",
      "Epoch 68/100 Iteration 1700| Training loss: 1.9643\n",
      "Epoch 69/100 Iteration 1710| Training loss: 1.9514\n",
      "Epoch 69/100 Iteration 1720| Training loss: 1.9846\n",
      "Epoch 70/100 Iteration 1730| Training loss: 1.9603\n",
      "Epoch 70/100 Iteration 1740| Training loss: 1.9618\n",
      "Epoch 70/100 Iteration 1750| Training loss: 1.9742\n",
      "Epoch 71/100 Iteration 1760| Training loss: 1.9358\n",
      "Epoch 71/100 Iteration 1770| Training loss: 1.9851\n",
      "Epoch 72/100 Iteration 1780| Training loss: 1.9589\n",
      "Epoch 72/100 Iteration 1790| Training loss: 1.9599\n",
      "Epoch 72/100 Iteration 1800| Training loss: 1.9492\n",
      "Epoch 73/100 Iteration 1810| Training loss: 1.9186\n",
      "Epoch 73/100 Iteration 1820| Training loss: 1.9833\n",
      "Epoch 74/100 Iteration 1830| Training loss: 1.9397\n",
      "Epoch 74/100 Iteration 1840| Training loss: 1.9483\n",
      "Epoch 74/100 Iteration 1850| Training loss: 1.9447\n",
      "Epoch 75/100 Iteration 1860| Training loss: 1.9173\n",
      "Epoch 75/100 Iteration 1870| Training loss: 1.9672\n",
      "Epoch 76/100 Iteration 1880| Training loss: 1.9359\n",
      "Epoch 76/100 Iteration 1890| Training loss: 1.9457\n",
      "Epoch 76/100 Iteration 1900| Training loss: 1.9389\n",
      "Epoch 77/100 Iteration 1910| Training loss: 1.9083\n",
      "Epoch 77/100 Iteration 1920| Training loss: 1.9628\n",
      "Epoch 78/100 Iteration 1930| Training loss: 1.9336\n",
      "Epoch 78/100 Iteration 1940| Training loss: 1.9376\n",
      "Epoch 78/100 Iteration 1950| Training loss: 1.9282\n",
      "Epoch 79/100 Iteration 1960| Training loss: 1.9025\n",
      "Epoch 79/100 Iteration 1970| Training loss: 1.9555\n",
      "Epoch 80/100 Iteration 1980| Training loss: 1.9257\n",
      "Epoch 80/100 Iteration 1990| Training loss: 1.9331\n",
      "Epoch 80/100 Iteration 2000| Training loss: 1.9191\n",
      "Epoch 81/100 Iteration 2010| Training loss: 1.9045\n",
      "Epoch 81/100 Iteration 2020| Training loss: 1.9512\n",
      "Epoch 82/100 Iteration 2030| Training loss: 1.9239\n",
      "Epoch 82/100 Iteration 2040| Training loss: 1.9266\n",
      "Epoch 82/100 Iteration 2050| Training loss: 1.9144\n",
      "Epoch 83/100 Iteration 2060| Training loss: 1.8937\n",
      "Epoch 83/100 Iteration 2070| Training loss: 1.9406\n",
      "Epoch 84/100 Iteration 2080| Training loss: 1.9182\n",
      "Epoch 84/100 Iteration 2090| Training loss: 1.9142\n",
      "Epoch 84/100 Iteration 2100| Training loss: 1.9058\n",
      "Epoch 85/100 Iteration 2110| Training loss: 1.8816\n",
      "Epoch 85/100 Iteration 2120| Training loss: 1.9365\n",
      "Epoch 86/100 Iteration 2130| Training loss: 1.9108\n",
      "Epoch 86/100 Iteration 2140| Training loss: 1.9040\n",
      "Epoch 86/100 Iteration 2150| Training loss: 1.9006\n",
      "Epoch 87/100 Iteration 2160| Training loss: 1.8716\n",
      "Epoch 87/100 Iteration 2170| Training loss: 1.9363\n",
      "Epoch 88/100 Iteration 2180| Training loss: 1.8954\n",
      "Epoch 88/100 Iteration 2190| Training loss: 1.9133\n",
      "Epoch 88/100 Iteration 2200| Training loss: 1.8900\n",
      "Epoch 89/100 Iteration 2210| Training loss: 1.8786\n",
      "Epoch 89/100 Iteration 2220| Training loss: 1.9271\n",
      "Epoch 90/100 Iteration 2230| Training loss: 1.8903\n",
      "Epoch 90/100 Iteration 2240| Training loss: 1.9019\n",
      "Epoch 90/100 Iteration 2250| Training loss: 1.8985\n",
      "Epoch 91/100 Iteration 2260| Training loss: 1.8680\n",
      "Epoch 91/100 Iteration 2270| Training loss: 1.9086\n",
      "Epoch 92/100 Iteration 2280| Training loss: 1.8804\n",
      "Epoch 92/100 Iteration 2290| Training loss: 1.8783\n",
      "Epoch 92/100 Iteration 2300| Training loss: 1.8869\n",
      "Epoch 93/100 Iteration 2310| Training loss: 1.8542\n",
      "Epoch 93/100 Iteration 2320| Training loss: 1.9219\n",
      "Epoch 94/100 Iteration 2330| Training loss: 1.8723\n",
      "Epoch 94/100 Iteration 2340| Training loss: 1.8904\n",
      "Epoch 94/100 Iteration 2350| Training loss: 1.8784\n",
      "Epoch 95/100 Iteration 2360| Training loss: 1.8570\n",
      "Epoch 95/100 Iteration 2370| Training loss: 1.9012\n",
      "Epoch 96/100 Iteration 2380| Training loss: 1.8632\n",
      "Epoch 96/100 Iteration 2390| Training loss: 1.8776\n",
      "Epoch 96/100 Iteration 2400| Training loss: 1.8887\n",
      "Epoch 97/100 Iteration 2410| Training loss: 1.8500\n",
      "Epoch 97/100 Iteration 2420| Training loss: 1.9096\n",
      "Epoch 98/100 Iteration 2430| Training loss: 1.8604\n",
      "Epoch 98/100 Iteration 2440| Training loss: 1.8818\n",
      "Epoch 98/100 Iteration 2450| Training loss: 1.8683\n",
      "Epoch 99/100 Iteration 2460| Training loss: 1.8397\n",
      "Epoch 99/100 Iteration 2470| Training loss: 1.8865\n",
      "Epoch 100/100 Iteration 2480| Training loss: 1.8669\n",
      "Epoch 100/100 Iteration 2490| Training loss: 1.8779\n",
      "Epoch 100/100 Iteration 2500| Training loss: 1.8518\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_steps = 100\n",
    "\n",
    "train_x, train_y = reshape_data(text_ints, batch_size, num_steps)\n",
    "\n",
    "rnn = CharRNN(num_classes=len(chars), batch_size=batch_size)\n",
    "rnn.train(train_x, train_y, num_epochs=100, ckpt_dir='./ch16-model2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  <<  x_onehot  >>  Tensor(\"one_hot:0\", shape=(1, 1, 68), dtype=float32)\n",
      "  <<  y_onehot  >>  Tensor(\"one_hot_1:0\", shape=(1, 1, 68), dtype=float32)\n",
      "  <<  lstm_outputs  >>  Tensor(\"rnn/transpose_1:0\", shape=(1, 1, 128), dtype=float32)\n",
      "INFO:tensorflow:Restoring parameters from ./ch16-model2/language_modeling.ckpt\n",
      "The horee and merse to the Seere\n",
      "\n",
      "   Ham. He tould a to the that he will to thy too merence\n",
      "\n",
      "   Ham. Well sim heare the Somnes in the preauth, with thes as it me the to that,\n",
      "That so this thy were is me me, and whing it sond, thay,\n",
      "As my his mad a dond at homan sout some\n",
      "\n",
      "   Ophe. Is stay and marnt and blyowne of the King,\n",
      "And thas war the Connous, in in toust thougherter him.\n",
      "What a whith stones thou mat sellost it my teene\n",
      " he door to be that\n",
      "  Poos. I my Larde hither, thay shere his mad and\n",
      "Hant t\n"
     ]
    }
   ],
   "source": [
    "del rnn\n",
    "\n",
    "np.random.seed(123)\n",
    "rnn = CharRNN(len(chars), sampling=True)\n",
    "print(rnn.sample(ckpt_dir='./ch16-model2/', output_length=500))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
